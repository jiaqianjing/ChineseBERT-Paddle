{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f35568-ab71-4cbd-b992-3cc4ea005f2c",
   "metadata": {},
   "source": [
    "# 先导入 Torch 版本的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70492487-d3cd-42b3-8709-31ce402082e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 ChineseBert 源码的根目录，加入到 python sys.path\n",
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/ChineseBert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a757c0c3-7221-4634-b2cf-5796614197e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 clone paddlenlp 源码，加入到 python sys.path\n",
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/Paddle_ChineseBert/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e653e18-baec-4b8f-93e5-f0bc2363d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChineseBert Model\n",
    "from datasets.bert_dataset import BertDataset\n",
    "from models.modeling_glycebert import GlyceBertModel\n",
    "CHINESEBERT_PATH='./pretrain_models/torch/ChineseBERT-base/'\n",
    "tokenizer = BertDataset(CHINESEBERT_PATH)\n",
    "chinese_bert = GlyceBertModel.from_pretrained(CHINESEBERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee48c9cf-24ac-44fd-ae0b-b63c72493ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = chinese_bert.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68999205-76e5-4b66-8b1b-518d939ca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '我喜欢猫'\n",
    "input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)\n",
    "length = input_ids.shape[0]\n",
    "input_ids = input_ids.view(1, length)\n",
    "pinyin_ids = pinyin_ids.view(1, length, 8)\n",
    "output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f36dde-82c8-42f3-aed7-1296cc825dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f64d6-3f9c-4506-a14c-1579a1cfaa9a",
   "metadata": {},
   "source": [
    "## 对比 ChineseBert 和 Bert 的网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d663bb2-b777-40b3-bc2f-c32e170f7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoModelForMaskedLM\n",
    "torch_chinese_bert_keys = chinese_bert.state_dict().keys()\n",
    "torch_bert_keys = BertModel.from_pretrained(\"bert-base-chinese\").state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54feead-49f4-42e7-b240-f363dbc82d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.position_ids',\n",
       " 'embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.pinyin_embeddings.embedding.weight',\n",
       " 'embeddings.pinyin_embeddings.conv.weight',\n",
       " 'embeddings.pinyin_embeddings.conv.bias',\n",
       " 'embeddings.glyph_embeddings.embedding.weight',\n",
       " 'embeddings.glyph_map.weight',\n",
       " 'embeddings.glyph_map.bias',\n",
       " 'embeddings.map_fc.weight',\n",
       " 'embeddings.map_fc.bias',\n",
       " 'embeddings.LayerNorm.weight',\n",
       " 'embeddings.LayerNorm.bias',\n",
       " 'encoder.layer.0.attention.self.query.weight',\n",
       " 'encoder.layer.0.attention.self.query.bias',\n",
       " 'encoder.layer.0.attention.self.key.weight',\n",
       " 'encoder.layer.0.attention.self.key.bias',\n",
       " 'encoder.layer.0.attention.self.value.weight',\n",
       " 'encoder.layer.0.attention.self.value.bias',\n",
       " 'encoder.layer.0.attention.output.dense.weight',\n",
       " 'encoder.layer.0.attention.output.dense.bias',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.0.intermediate.dense.weight',\n",
       " 'encoder.layer.0.intermediate.dense.bias',\n",
       " 'encoder.layer.0.output.dense.weight',\n",
       " 'encoder.layer.0.output.dense.bias',\n",
       " 'encoder.layer.0.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.attention.self.query.weight',\n",
       " 'encoder.layer.1.attention.self.query.bias',\n",
       " 'encoder.layer.1.attention.self.key.weight',\n",
       " 'encoder.layer.1.attention.self.key.bias',\n",
       " 'encoder.layer.1.attention.self.value.weight',\n",
       " 'encoder.layer.1.attention.self.value.bias',\n",
       " 'encoder.layer.1.attention.output.dense.weight',\n",
       " 'encoder.layer.1.attention.output.dense.bias',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.intermediate.dense.weight',\n",
       " 'encoder.layer.1.intermediate.dense.bias',\n",
       " 'encoder.layer.1.output.dense.weight',\n",
       " 'encoder.layer.1.output.dense.bias',\n",
       " 'encoder.layer.1.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.attention.self.query.weight',\n",
       " 'encoder.layer.2.attention.self.query.bias',\n",
       " 'encoder.layer.2.attention.self.key.weight',\n",
       " 'encoder.layer.2.attention.self.key.bias',\n",
       " 'encoder.layer.2.attention.self.value.weight',\n",
       " 'encoder.layer.2.attention.self.value.bias',\n",
       " 'encoder.layer.2.attention.output.dense.weight',\n",
       " 'encoder.layer.2.attention.output.dense.bias',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.intermediate.dense.weight',\n",
       " 'encoder.layer.2.intermediate.dense.bias',\n",
       " 'encoder.layer.2.output.dense.weight',\n",
       " 'encoder.layer.2.output.dense.bias',\n",
       " 'encoder.layer.2.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.attention.self.query.weight',\n",
       " 'encoder.layer.3.attention.self.query.bias',\n",
       " 'encoder.layer.3.attention.self.key.weight',\n",
       " 'encoder.layer.3.attention.self.key.bias',\n",
       " 'encoder.layer.3.attention.self.value.weight',\n",
       " 'encoder.layer.3.attention.self.value.bias',\n",
       " 'encoder.layer.3.attention.output.dense.weight',\n",
       " 'encoder.layer.3.attention.output.dense.bias',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.intermediate.dense.weight',\n",
       " 'encoder.layer.3.intermediate.dense.bias',\n",
       " 'encoder.layer.3.output.dense.weight',\n",
       " 'encoder.layer.3.output.dense.bias',\n",
       " 'encoder.layer.3.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.attention.self.query.weight',\n",
       " 'encoder.layer.4.attention.self.query.bias',\n",
       " 'encoder.layer.4.attention.self.key.weight',\n",
       " 'encoder.layer.4.attention.self.key.bias',\n",
       " 'encoder.layer.4.attention.self.value.weight',\n",
       " 'encoder.layer.4.attention.self.value.bias',\n",
       " 'encoder.layer.4.attention.output.dense.weight',\n",
       " 'encoder.layer.4.attention.output.dense.bias',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.intermediate.dense.weight',\n",
       " 'encoder.layer.4.intermediate.dense.bias',\n",
       " 'encoder.layer.4.output.dense.weight',\n",
       " 'encoder.layer.4.output.dense.bias',\n",
       " 'encoder.layer.4.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.attention.self.query.weight',\n",
       " 'encoder.layer.5.attention.self.query.bias',\n",
       " 'encoder.layer.5.attention.self.key.weight',\n",
       " 'encoder.layer.5.attention.self.key.bias',\n",
       " 'encoder.layer.5.attention.self.value.weight',\n",
       " 'encoder.layer.5.attention.self.value.bias',\n",
       " 'encoder.layer.5.attention.output.dense.weight',\n",
       " 'encoder.layer.5.attention.output.dense.bias',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.intermediate.dense.weight',\n",
       " 'encoder.layer.5.intermediate.dense.bias',\n",
       " 'encoder.layer.5.output.dense.weight',\n",
       " 'encoder.layer.5.output.dense.bias',\n",
       " 'encoder.layer.5.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.attention.self.query.weight',\n",
       " 'encoder.layer.6.attention.self.query.bias',\n",
       " 'encoder.layer.6.attention.self.key.weight',\n",
       " 'encoder.layer.6.attention.self.key.bias',\n",
       " 'encoder.layer.6.attention.self.value.weight',\n",
       " 'encoder.layer.6.attention.self.value.bias',\n",
       " 'encoder.layer.6.attention.output.dense.weight',\n",
       " 'encoder.layer.6.attention.output.dense.bias',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.intermediate.dense.weight',\n",
       " 'encoder.layer.6.intermediate.dense.bias',\n",
       " 'encoder.layer.6.output.dense.weight',\n",
       " 'encoder.layer.6.output.dense.bias',\n",
       " 'encoder.layer.6.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.attention.self.query.weight',\n",
       " 'encoder.layer.7.attention.self.query.bias',\n",
       " 'encoder.layer.7.attention.self.key.weight',\n",
       " 'encoder.layer.7.attention.self.key.bias',\n",
       " 'encoder.layer.7.attention.self.value.weight',\n",
       " 'encoder.layer.7.attention.self.value.bias',\n",
       " 'encoder.layer.7.attention.output.dense.weight',\n",
       " 'encoder.layer.7.attention.output.dense.bias',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.intermediate.dense.weight',\n",
       " 'encoder.layer.7.intermediate.dense.bias',\n",
       " 'encoder.layer.7.output.dense.weight',\n",
       " 'encoder.layer.7.output.dense.bias',\n",
       " 'encoder.layer.7.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.attention.self.query.weight',\n",
       " 'encoder.layer.8.attention.self.query.bias',\n",
       " 'encoder.layer.8.attention.self.key.weight',\n",
       " 'encoder.layer.8.attention.self.key.bias',\n",
       " 'encoder.layer.8.attention.self.value.weight',\n",
       " 'encoder.layer.8.attention.self.value.bias',\n",
       " 'encoder.layer.8.attention.output.dense.weight',\n",
       " 'encoder.layer.8.attention.output.dense.bias',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.intermediate.dense.weight',\n",
       " 'encoder.layer.8.intermediate.dense.bias',\n",
       " 'encoder.layer.8.output.dense.weight',\n",
       " 'encoder.layer.8.output.dense.bias',\n",
       " 'encoder.layer.8.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.attention.self.query.weight',\n",
       " 'encoder.layer.9.attention.self.query.bias',\n",
       " 'encoder.layer.9.attention.self.key.weight',\n",
       " 'encoder.layer.9.attention.self.key.bias',\n",
       " 'encoder.layer.9.attention.self.value.weight',\n",
       " 'encoder.layer.9.attention.self.value.bias',\n",
       " 'encoder.layer.9.attention.output.dense.weight',\n",
       " 'encoder.layer.9.attention.output.dense.bias',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.intermediate.dense.weight',\n",
       " 'encoder.layer.9.intermediate.dense.bias',\n",
       " 'encoder.layer.9.output.dense.weight',\n",
       " 'encoder.layer.9.output.dense.bias',\n",
       " 'encoder.layer.9.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.attention.self.query.weight',\n",
       " 'encoder.layer.10.attention.self.query.bias',\n",
       " 'encoder.layer.10.attention.self.key.weight',\n",
       " 'encoder.layer.10.attention.self.key.bias',\n",
       " 'encoder.layer.10.attention.self.value.weight',\n",
       " 'encoder.layer.10.attention.self.value.bias',\n",
       " 'encoder.layer.10.attention.output.dense.weight',\n",
       " 'encoder.layer.10.attention.output.dense.bias',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.intermediate.dense.weight',\n",
       " 'encoder.layer.10.intermediate.dense.bias',\n",
       " 'encoder.layer.10.output.dense.weight',\n",
       " 'encoder.layer.10.output.dense.bias',\n",
       " 'encoder.layer.10.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.attention.self.query.weight',\n",
       " 'encoder.layer.11.attention.self.query.bias',\n",
       " 'encoder.layer.11.attention.self.key.weight',\n",
       " 'encoder.layer.11.attention.self.key.bias',\n",
       " 'encoder.layer.11.attention.self.value.weight',\n",
       " 'encoder.layer.11.attention.self.value.bias',\n",
       " 'encoder.layer.11.attention.output.dense.weight',\n",
       " 'encoder.layer.11.attention.output.dense.bias',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.intermediate.dense.weight',\n",
       " 'encoder.layer.11.intermediate.dense.bias',\n",
       " 'encoder.layer.11.output.dense.weight',\n",
       " 'encoder.layer.11.output.dense.bias',\n",
       " 'encoder.layer.11.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.output.LayerNorm.bias',\n",
       " 'pooler.dense.weight',\n",
       " 'pooler.dense.bias']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全部的 ChineseBert 的参数\n",
    "list(torch_chinese_bert_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1fb4b6-1aca-41ae-a9d6-86ec949ff5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings.glyph_embeddings.embedding.weight',\n",
       " 'embeddings.glyph_map.bias',\n",
       " 'embeddings.glyph_map.weight',\n",
       " 'embeddings.map_fc.bias',\n",
       " 'embeddings.map_fc.weight',\n",
       " 'embeddings.pinyin_embeddings.conv.bias',\n",
       " 'embeddings.pinyin_embeddings.conv.weight',\n",
       " 'embeddings.pinyin_embeddings.embedding.weight'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChineseBert 多出来的参数（embedding 层）\n",
    "set(list(torch_chinese_bert_keys)) - set(list(torch_bert_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b098c919-8630-4082-adf9-af3808ce288b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChineseBert 少的参数\n",
    "set(list(torch_bert_keys)) - set(list(torch_chinese_bert_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9221c8-118b-4278-9ada-1330b2ad7903",
   "metadata": {},
   "source": [
    "## 模型参数 torch 转换 paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b10e4c-cdd9-4b39-b2de-6453e0635cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  bert.embeddings.position_ids \t torch.Size([1, 512])\n",
      "p:  bert.embeddings.position_ids \t [1, 512] \n",
      "\n",
      "t:  bert.embeddings.word_embeddings.weight \t torch.Size([23236, 1024])\n",
      "p:  bert.embeddings.word_embeddings.weight \t [23236, 1024] \n",
      "\n",
      "t:  bert.embeddings.position_embeddings.weight \t torch.Size([512, 1024])\n",
      "p:  bert.embeddings.position_embeddings.weight \t [512, 1024] \n",
      "\n",
      "t:  bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 1024])\n",
      "p:  bert.embeddings.token_type_embeddings.weight \t [2, 1024] \n",
      "\n",
      "t:  bert.embeddings.pinyin_embeddings.embedding.weight \t torch.Size([32, 128])\n",
      "p:  bert.embeddings.pinyin_embeddings.embedding.weight \t [32, 128] \n",
      "\n",
      "t:  bert.embeddings.pinyin_embeddings.conv.weight \t torch.Size([1024, 128, 2])\n",
      "p:  bert.embeddings.pinyin_embeddings.conv.weight \t [1024, 128, 2] \n",
      "\n",
      "t:  bert.embeddings.pinyin_embeddings.conv.bias \t torch.Size([1024])\n",
      "p:  bert.embeddings.pinyin_embeddings.conv.bias \t [1024] \n",
      "\n",
      "t:  bert.embeddings.glyph_embeddings.embedding.weight \t torch.Size([23236, 1728])\n",
      "p:  bert.embeddings.glyph_embeddings.embedding.weight \t [23236, 1728] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.embeddings.glyph_map.weight \t torch.Size([1024, 1728])\n",
      "p:  bert.embeddings.glyph_map.weight \t [1728, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.embeddings.glyph_map.bias \t torch.Size([1024])\n",
      "p:  bert.embeddings.glyph_map.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.embeddings.map_fc.weight \t torch.Size([1024, 3072])\n",
      "p:  bert.embeddings.map_fc.weight \t [3072, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.embeddings.map_fc.bias \t torch.Size([1024])\n",
      "p:  bert.embeddings.map_fc.bias \t [1024] \n",
      "\n",
      "t:  bert.embeddings.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.embeddings.LayerNorm.weight \t [1024] \n",
      "\n",
      "t:  bert.embeddings.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.embeddings.LayerNorm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.0.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.0.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.0.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.0.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.0.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.0.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.0.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.0.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.0.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.1.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.1.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.1.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.1.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.1.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.1.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.1.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.1.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.1.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.2.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.2.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.2.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.2.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.2.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.2.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.2.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.2.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.2.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.3.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.3.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.3.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.3.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.3.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.3.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.3.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.3.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.3.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.4.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.4.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.4.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.4.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.4.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.4.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.4.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.4.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.4.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.5.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.5.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.5.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.5.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.5.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.5.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.5.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.5.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.5.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.6.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.6.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.6.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.6.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.6.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.6.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.6.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.6.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.6.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.7.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.7.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.7.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.7.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.7.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.7.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.7.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.7.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.7.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.8.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.8.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.8.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.8.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.8.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.8.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.8.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.8.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.8.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.9.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.9.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.9.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.9.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.9.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.9.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.9.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.9.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.9.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.10.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.10.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.10.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.10.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.10.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.10.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.10.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.10.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.10.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.11.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.11.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.11.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.11.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.11.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.11.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.11.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.11.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.11.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.12.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.12.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.12.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.12.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.12.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.12.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.12.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.12.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.12.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.12.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.12.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.12.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.12.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.13.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.13.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.13.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.13.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.13.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.13.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.13.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.13.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.13.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.13.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.13.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.13.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.13.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.14.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.14.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.14.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.14.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.14.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.14.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.14.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.14.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.14.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.14.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.14.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.14.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.14.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.15.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.15.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.15.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.15.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.15.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.15.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.15.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.15.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.15.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.15.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.15.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.15.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.15.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.16.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.16.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.16.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.16.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.16.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.16.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.16.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.16.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.16.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.16.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.16.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.16.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.16.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.17.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.17.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.17.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.17.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.17.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.17.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.17.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.17.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.17.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.17.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.17.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.17.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.17.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.18.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.18.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.18.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.18.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.18.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.18.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.18.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.18.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.18.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.18.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.18.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.18.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.18.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.19.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.19.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.19.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.19.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.19.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.19.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.19.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.19.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.19.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.19.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.19.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.19.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.19.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.20.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.20.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.20.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.20.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.20.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.20.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.20.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.20.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.20.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.20.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.20.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.20.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.20.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.21.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.21.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.21.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.21.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.21.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.21.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.21.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.21.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.21.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.21.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.21.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.21.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.21.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.22.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.22.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.22.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.22.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.22.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.22.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.22.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.22.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.22.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.22.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.22.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.22.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.22.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.query.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.23.self_attn.q_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.query.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.self_attn.q_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.key.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.23.self_attn.k_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.key.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.self_attn.k_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.value.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.23.self_attn.v_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.self.value.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.self_attn.v_proj.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.output.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.encoder.layers.23.self_attn.out_proj.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.attention.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.self_attn.out_proj.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.23.attention.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.norm1.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.23.attention.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.norm1.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  bert.encoder.layers.23.linear1.weight \t [1024, 4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  bert.encoder.layers.23.linear1.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  bert.encoder.layers.23.linear2.weight \t [4096, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.encoder.layer.23.output.dense.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.linear2.bias \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.23.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.norm2.weight \t [1024] \n",
      "\n",
      "t:  bert.encoder.layer.23.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  bert.encoder.layers.23.norm2.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  bert.pooler.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  bert.pooler.dense.weight \t [1024, 1024] \n",
      "\n",
      "t:  bert.pooler.dense.bias \t torch.Size([1024])\n",
      "p:  bert.pooler.dense.bias \t [1024] \n",
      "\n",
      "t:  cls.predictions.bias \t torch.Size([23236])\n",
      "p:  cls.predictions.bias \t [23236] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  cls.predictions.transform.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  cls.predictions.transform.dense.weight \t [1024, 1024] \n",
      "\n",
      "t:  cls.predictions.transform.dense.bias \t torch.Size([1024])\n",
      "p:  cls.predictions.transform.dense.bias \t [1024] \n",
      "\n",
      "t:  cls.predictions.transform.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  cls.predictions.transform.LayerNorm.weight \t [1024] \n",
      "\n",
      "t:  cls.predictions.transform.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  cls.predictions.transform.LayerNorm.bias \t [1024] \n",
      "\n",
      "t:  cls.predictions.decoder.weight \t torch.Size([23236, 1024])\n",
      "p:  cls.predictions.decoder.weight \t [23236, 1024] \n",
      "\n",
      "t:  cls.predictions.decoder.bias \t torch.Size([23236])\n",
      "p:  cls.predictions.decoder.bias \t [23236] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ChineseBERT-base:  ./pretrain_models/torch/ChineseBERT-base/pytorch_model.bin\n",
    "# ChineseBERT-large: ./pretrain_models/torch/ChineseBERT-large/pytorch_model.bin\n",
    "torch_model_path = \"./pretrain_models/torch/ChineseBERT-large/pytorch_model.bin\"\n",
    "torch_state_dict = torch.load(torch_model_path)\n",
    "paddle_model_path = \"./pretrain_models/paddle/ChineseBERT-large/model_state.pdparams\"\n",
    "paddle_state_dict = {}\n",
    "\n",
    "# State_dict's keys mapping: from torch to paddle\n",
    "keys_dict = {\n",
    "    # about encoder layer\n",
    "    'encoder.layer': 'encoder.layers',\n",
    "    'attention.self.query': 'self_attn.q_proj', # 需要转置\n",
    "    'attention.self.key': 'self_attn.k_proj',   # 需要转置\n",
    "    'attention.self.value': 'self_attn.v_proj', # 需要转置\n",
    "    'attention.output.dense': 'self_attn.out_proj',  # 需要转置\n",
    "    'attention.output.LayerNorm': 'norm1', # 需要转置\n",
    "    'intermediate.dense': 'linear1', # 需要转置\n",
    "    'output.dense': 'linear2', # 需要转置\n",
    "    'output.LayerNorm': 'norm2', # 需要转置\n",
    "}\n",
    "\n",
    "\n",
    "for torch_key in torch_state_dict:\n",
    "    paddle_key = torch_key\n",
    "    for k in keys_dict:\n",
    "        if k in paddle_key:\n",
    "            paddle_key = paddle_key.replace(k, keys_dict[k])\n",
    "\n",
    "    if ('map_fc' in paddle_key) or ('glyph_map' in paddle_key) or ('linear' in paddle_key) or ('proj' in  paddle_key) or ('vocab' in  paddle_key and 'weight' in  paddle_key) or (\"dense.weight\" in paddle_key) or ('transform.weight' in paddle_key) or ('seq_relationship.weight' in paddle_key):\n",
    "        print(\"transpose(permute) ---------->\")\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy().transpose())\n",
    "    else:\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy())\n",
    "\n",
    "    print(\"t: \", torch_key,\"\\t\", torch_state_dict[torch_key].shape)\n",
    "    print(\"p: \", paddle_key, \"\\t\", paddle_state_dict[paddle_key].shape, \"\\n\")\n",
    "\n",
    "paddle.save(paddle_state_dict, paddle_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0a081-e2f7-4a52-9a5d-0b32f0bf3bd5",
   "metadata": {},
   "source": [
    "# 使用 Paddle 转写 forward\n",
    "* 我们按照源码的组成部分分批转写"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213a85e-b0ff-480f-8a5c-1e8e8f4df1b7",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b28ac4-942c-4dac-9db9-1cfe2800807e",
   "metadata": {},
   "source": [
    "### 对齐 Bert tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2984c5ca-73f3-4645-94a6-cf1fe583d5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ PyTorch ==================\n",
      "[101, 2769, 1599, 3614, 4344, 102]\n",
      "['[CLS]', '我', '喜', '欢', '猫', '[SEP]']\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (0, 0)]\n",
      "============= Paddle =================\n",
      "[101, 2769, 1599, 3614, 4344, 102]\n",
      "['[CLS]', '我', '喜', '欢', '猫', '[SEP]']\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 对齐 paddle 和 torch 的 bert tokenizer \n",
    "# =========================================\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from paddlenlp.transformers import BertTokenizer\n",
    "\n",
    "torch_token = BertWordPieceTokenizer('./pretrain_models/torch/ChineseBERT-base/vocab.txt')\n",
    "berttokenizer = BertTokenizer('./pretrain_models/torch/ChineseBERT-base/vocab.txt')\n",
    "\n",
    "sentence=\"我喜欢猫\"\n",
    "\n",
    "# torch\n",
    "print(\"============ PyTorch ==================\")\n",
    "bert_tokens = torch_token.encode(sentence)\n",
    "print(bert_tokens.ids)\n",
    "print(bert_tokens.tokens)\n",
    "print(bert_tokens.offsets)\n",
    "\n",
    "print(\"============= Paddle =================\")\n",
    "\n",
    "# paddle\n",
    "# ids\n",
    "p_bert_tokens = berttokenizer.encode(sentence)\n",
    "p_bert_tokens_ids = p_bert_tokens['input_ids']\n",
    "print(p_bert_tokens_ids)\n",
    "\n",
    "# tokens\n",
    "p_bert_tokens_tokens = berttokenizer.tokenize(sentence)\n",
    "p_bert_tokens_tokens.insert(0, '[CLS]')\n",
    "p_bert_tokens_tokens.append('[SEP]')\n",
    "print(p_bert_tokens_tokens)\n",
    "\n",
    "# offsets\n",
    "p_bert_tokens_offsets = berttokenizer.get_offset_mapping(sentence)\n",
    "p_bert_tokens_offsets.insert(0, (0, 0))\n",
    "p_bert_tokens_offsets.append((0, 0))\n",
    "print(p_bert_tokens_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61f292-bd87-4305-b8de-d590ad133467",
   "metadata": {},
   "source": [
    "### 对齐 ChineseBert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d4baf33-0f71-4d17-bb9f-3c35de3e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# PyTorch 的 ChineseBert BertDataset (tokenizer)\n",
    "# =================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import tokenizers\n",
    "import torch\n",
    "from pypinyin import pinyin, Style\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "class BertDataset(object):\n",
    "\n",
    "    def __init__(self, bert_path, max_length: int = 512):\n",
    "        super().__init__()\n",
    "        vocab_file = os.path.join(bert_path, 'vocab.txt')\n",
    "        config_path = os.path.join(bert_path, 'config')\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "\n",
    "        # load pinyin map dict\n",
    "        with open(os.path.join(config_path, 'pinyin_map.json'), encoding='utf8') as fin:\n",
    "            self.pinyin_dict = json.load(fin)\n",
    "        # load char id map tensor\n",
    "        with open(os.path.join(config_path, 'id2pinyin.json'), encoding='utf8') as fin:\n",
    "            self.id2pinyin = json.load(fin)\n",
    "        # load pinyin map tensor\n",
    "        with open(os.path.join(config_path, 'pinyin2tensor.json'), encoding='utf8') as fin:\n",
    "            self.pinyin2tensor = json.load(fin)\n",
    "\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        # convert sentence to ids\n",
    "        tokenizer_output = self.tokenizer.encode(sentence)\n",
    "        bert_tokens = tokenizer_output.ids\n",
    "        pinyin_tokens = self.convert_sentence_to_pinyin_ids(sentence, tokenizer_output)\n",
    "        # assert，token nums should be same as pinyin token nums\n",
    "        assert len(bert_tokens) <= self.max_length\n",
    "        assert len(bert_tokens) == len(pinyin_tokens)\n",
    "        # convert list to tensor\n",
    "        input_ids = torch.LongTensor(bert_tokens)\n",
    "        pinyin_ids = torch.LongTensor(pinyin_tokens).view(-1)\n",
    "        return input_ids, pinyin_ids\n",
    "\n",
    "    def convert_sentence_to_pinyin_ids(self, sentence: str, tokenizer_output: tokenizers.Encoding) -> List[List[int]]:\n",
    "        # get pinyin of a sentence\n",
    "        pinyin_list = pinyin(sentence, style=Style.TONE3, heteronym=True, errors=lambda x: [['not chinese'] for _ in x])\n",
    "        pinyin_locs = {}\n",
    "        # get pinyin of each location\n",
    "        for index, item in enumerate(pinyin_list):\n",
    "            pinyin_string = item[0]\n",
    "            # not a Chinese character, pass\n",
    "            if pinyin_string == \"not chinese\":\n",
    "                continue\n",
    "            if pinyin_string in self.pinyin2tensor:\n",
    "                pinyin_locs[index] = self.pinyin2tensor[pinyin_string]\n",
    "            else:\n",
    "                ids = [0] * 8\n",
    "                for i, p in enumerate(pinyin_string):\n",
    "                    if p not in self.pinyin_dict[\"char2idx\"]:\n",
    "                        ids = [0] * 8\n",
    "                        break\n",
    "                    ids[i] = self.pinyin_dict[\"char2idx\"][p]\n",
    "                pinyin_locs[index] = ids\n",
    "\n",
    "        # find chinese character location, and generate pinyin ids\n",
    "        pinyin_ids = []\n",
    "        for idx, (token, offset) in enumerate(zip(tokenizer_output.tokens, tokenizer_output.offsets)):\n",
    "            if offset[1] - offset[0] != 1:\n",
    "                pinyin_ids.append([0] * 8)\n",
    "                continue\n",
    "            if offset[0] in pinyin_locs:\n",
    "                pinyin_ids.append(pinyin_locs[offset[0]])\n",
    "            else:\n",
    "                pinyin_ids.append([0] * 8)\n",
    "\n",
    "        return pinyin_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69640569-84b6-4017-9945-f3b64585a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# Paddle 的 ChineseBert BertDataset (tokenizer)\n",
    "# =================================================\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from pypinyin import pinyin, Style\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.transformers import BertTokenizer\n",
    "\n",
    "class PaddleBertDataset(object):\n",
    "    def __init__(self, bert_path, max_length: int = 512):\n",
    "        super().__init__()\n",
    "        vocab_file = os.path.join(bert_path, 'vocab.txt')\n",
    "        config_path = os.path.join(bert_path, 'config')\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer(vocab_file)\n",
    "        \n",
    "        # load pinyin map dict\n",
    "        with open(os.path.join(config_path, 'pinyin_map.json'), encoding='utf8') as fin:\n",
    "            self.pinyin_dict = json.load(fin)\n",
    "        # load char id map tensor\n",
    "        with open(os.path.join(config_path, 'id2pinyin.json'), encoding='utf8') as fin:\n",
    "            self.id2pinyin = json.load(fin)\n",
    "        # load pinyin map tensor\n",
    "        with open(os.path.join(config_path, 'pinyin2tensor.json'), encoding='utf8') as fin:\n",
    "            self.pinyin2tensor = json.load(fin)\n",
    "    \n",
    "    def tokenize_sentence(self, sentence):\n",
    "        # convert sentence to ids\n",
    "        tokenizer_output = self.tokenizer.encode(sentence)\n",
    "        bert_tokens = tokenizer_output['input_ids']\n",
    "        pinyin_tokens = self.convert_sentence_to_pinyin_ids(sentence)\n",
    "        # assert，token nums should be same as pinyin token nums\n",
    "        assert len(bert_tokens) <= self.max_length\n",
    "        assert len(bert_tokens) == len(pinyin_tokens)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        input_ids = paddle.to_tensor(bert_tokens)\n",
    "        pinyin_ids = paddle.to_tensor(pinyin_tokens).reshape([-1])\n",
    "        return input_ids, pinyin_ids\n",
    "\n",
    "    def convert_sentence_to_pinyin_ids(self, sentence: str) -> List[List[int]]:\n",
    "        # get offsets\n",
    "        bert_tokens_offsets = self.tokenizer.get_offset_mapping(sentence)\n",
    "        bert_tokens_offsets.insert(0, (0, 0))\n",
    "        bert_tokens_offsets.append((0, 0))\n",
    "        \n",
    "        # get tokens\n",
    "        bert_tokens_tokens = self.tokenizer.tokenize(sentence)\n",
    "        bert_tokens_tokens.insert(0, '[CLS]')\n",
    "        bert_tokens_tokens.append('[SEP]')\n",
    "        \n",
    "        # get pinyin of a sentence\n",
    "        pinyin_list = pinyin(sentence, style=Style.TONE3, heteronym=True, errors=lambda x: [['not chinese'] for _ in x])\n",
    "        pinyin_locs = {}\n",
    "        # get pinyin of each location\n",
    "        for index, item in enumerate(pinyin_list):\n",
    "            pinyin_string = item[0]\n",
    "            # not a Chinese character, pass\n",
    "            if pinyin_string == \"not chinese\":\n",
    "                continue\n",
    "            if pinyin_string in self.pinyin2tensor:\n",
    "                pinyin_locs[index] = self.pinyin2tensor[pinyin_string]\n",
    "            else:\n",
    "                ids = [0] * 8\n",
    "                for i, p in enumerate(pinyin_string):\n",
    "                    if p not in self.pinyin_dict[\"char2idx\"]:\n",
    "                        ids = [0] * 8\n",
    "                        break\n",
    "                    ids[i] = self.pinyin_dict[\"char2idx\"][p]\n",
    "                pinyin_locs[index] = ids\n",
    "\n",
    "        # find chinese character location, and generate pinyin ids\n",
    "        pinyin_ids = []\n",
    "        for idx, (token, offset) in enumerate(zip(bert_tokens_tokens, bert_tokens_offsets)):\n",
    "            if offset[1] - offset[0] != 1:\n",
    "                pinyin_ids.append([0] * 8)\n",
    "                continue\n",
    "            if offset[0] in pinyin_locs:\n",
    "                pinyin_ids.append(pinyin_locs[offset[0]])\n",
    "            else:\n",
    "                pinyin_ids.append([0] * 8)\n",
    "\n",
    "        return pinyin_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd69eebe-1525-4c04-88dd-891cbf6c0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== torch =============================\n",
      "[ 101 2769 1599 3614 4344  102] [ 0  0  0  0  0  0  0  0 28 20  3  0  0  0  0  0 29 14  3  0  0  0  0  0\n",
      " 13 26  6 19  1  0  0  0 18  6 20  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 23:18:46.616715 23468 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 6.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0912 23:18:46.620927 23468 device_context.cc:422] device: 0, cuDNN Version: 8.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== paddle =============================\n",
      "[ 101 2769 1599 3614 4344  102] [ 0  0  0  0  0  0  0  0 28 20  3  0  0  0  0  0 29 14  3  0  0  0  0  0\n",
      " 13 26  6 19  1  0  0  0 18  6 20  1  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "sentence = '我喜欢猫'\n",
    "tokenizer = BertDataset(CHINESEBERT_PATH)\n",
    "input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)\n",
    "print(\"============================== torch =============================\")\n",
    "print(input_ids.cpu().detach().numpy(), pinyin_ids.cpu().detach().numpy())\n",
    "print()\n",
    "\n",
    "# paddle\n",
    "sentence = '我喜欢猫'\n",
    "paddle_tokenizer = PaddleBertDataset(CHINESEBERT_PATH)\n",
    "paddle_input_ids, paddle_pinyin_ids = paddle_tokenizer.tokenize_sentence(sentence)\n",
    "print(\"============================== paddle =============================\")\n",
    "print(paddle_input_ids.cpu().detach().numpy(), paddle_pinyin_ids.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ee6a9-d2c9-40de-90e4-3e983d6782d9",
   "metadata": {},
   "source": [
    "## PinyinEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15c1135-e40f-4f39-9b81-dad140c4dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch PinyinEmbedding\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class PinyinEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size: int, pinyin_out_dim: int, config_path):\n",
    "        \"\"\"\n",
    "            Pinyin Embedding Module\n",
    "        Args:\n",
    "            embedding_size: the size of each embedding vector\n",
    "            pinyin_out_dim: kernel number of conv\n",
    "        \"\"\"\n",
    "        super(PinyinEmbedding, self).__init__()\n",
    "        with open(os.path.join(config_path, 'pinyin_map.json')) as fin:\n",
    "            pinyin_dict = json.load(fin)\n",
    "        self.pinyin_out_dim = pinyin_out_dim\n",
    "        self.embedding = nn.Embedding(len(pinyin_dict['idx2char']), embedding_size)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_size, \n",
    "                              out_channels=self.pinyin_out_dim, \n",
    "                              kernel_size=2,\n",
    "                              stride=1, \n",
    "                              padding=0)\n",
    "\n",
    "    def forward(self, pinyin_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pinyin_ids: (bs*sentence_length*pinyin_locs)\n",
    "\n",
    "        Returns:\n",
    "            pinyin_embed: (bs,sentence_length,pinyin_out_dim)\n",
    "        \"\"\"\n",
    "        # input pinyin ids for 1-D conv\n",
    "        embed = self.embedding(pinyin_ids)  # [bs,sentence_length,pinyin_locs,embed_size]\n",
    "        bs, sentence_length, pinyin_locs, embed_size = embed.shape\n",
    "        view_embed = embed.view(-1, pinyin_locs, embed_size)  # [(bs*sentence_length),pinyin_locs,embed_size]\n",
    "        input_embed = view_embed.permute(0, 2, 1)  # [(bs*sentence_length), embed_size, pinyin_locs]\n",
    "        # conv + max_pooling\n",
    "        pinyin_conv = self.conv(input_embed)  # [(bs*sentence_length),pinyin_out_dim,H]\n",
    "        pinyin_embed = F.max_pool1d(pinyin_conv, pinyin_conv.shape[-1])  # [(bs*sentence_length),pinyin_out_dim,1]\n",
    "        return pinyin_embed.view(bs, sentence_length, self.pinyin_out_dim)  # [bs,sentence_length,pinyin_out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b80c94d-9824-40f4-b7be-000dde6f0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paddle PinyinEmbedding\n",
    "\n",
    "import json\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "\n",
    "class PaddlePinyinEmbedding(paddle.nn.Layer):\n",
    "    def __init__(self, embedding_size: int, pinyin_out_dim: int, config_path):\n",
    "        \"\"\"\n",
    "            Pinyin Embedding Module\n",
    "        Args:\n",
    "            embedding_size: the size of each embedding vector\n",
    "            pinyin_out_dim: kernel number of conv\n",
    "        \"\"\"\n",
    "        super(PaddlePinyinEmbedding, self).__init__()\n",
    "        with open(os.path.join(config_path, 'pinyin_map.json')) as fin:\n",
    "            pinyin_dict = json.load(fin)\n",
    "        self.pinyin_out_dim = pinyin_out_dim\n",
    "        self.embedding = paddle.nn.Embedding(len(pinyin_dict['idx2char']), embedding_size)\n",
    "        self.conv = paddle.nn.Conv1D(in_channels=embedding_size, \n",
    "                                     out_channels=self.pinyin_out_dim, \n",
    "                                     kernel_size=2,\n",
    "                                     stride=1, \n",
    "                                     padding=0,\n",
    "                                     bias_attr=True)\n",
    "\n",
    "    def forward(self, pinyin_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pinyin_ids: (bs*sentence_length*pinyin_locs)\n",
    "\n",
    "        Returns:\n",
    "            pinyin_embed: (bs,sentence_length,pinyin_out_dim)\n",
    "        \"\"\"\n",
    "        # input pinyin ids for 1-D conv\n",
    "        embed = self.embedding(pinyin_ids)  # [bs,sentence_length,pinyin_locs,embed_size]\n",
    "        bs, sentence_length, pinyin_locs, embed_size = embed.shape\n",
    "        view_embed = embed.reshape((-1, pinyin_locs, embed_size))  # [(bs*sentence_length),pinyin_locs,embed_size]\n",
    "        input_embed = view_embed.transpose([0, 2, 1])  # [(bs*sentence_length), embed_size, pinyin_locs]\n",
    "        # conv + max_pooling\n",
    "        pinyin_conv = self.conv(input_embed)  # [(bs*sentence_length),pinyin_out_dim,H]\n",
    "        pinyin_embed = paddle.nn.functional.max_pool1d(pinyin_conv, pinyin_conv.shape[-1])  # [(bs*sentence_length),pinyin_out_dim,1]\n",
    "        return pinyin_embed.reshape((bs, sentence_length, self.pinyin_out_dim))  # [bs,sentence_length,pinyin_out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b42e99f-7632-467b-914c-910a78b75db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== torch =============================\n",
      "length: 6\n",
      "torch size: torch.Size([6])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0, 28, 20,  3,  0,  0,  0,  0,  0, 29, 14,\n",
      "         3,  0,  0,  0,  0,  0, 13, 26,  6, 19,  1,  0,  0,  0, 18,  6, 20,  1,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [28, 20,  3,  0,  0,  0,  0,  0],\n",
      "         [29, 14,  3,  0,  0,  0,  0,  0],\n",
      "         [13, 26,  6, 19,  1,  0,  0,  0],\n",
      "         [18,  6, 20,  1,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0]]]) torch.Size([1, 6, 8])\n",
      ">>>>torch_pinyin_emb<<<<\n",
      "tensor([[[-0.6347, -0.2307, -0.9765,  ..., -0.5059,  0.1007,  0.6182],\n",
      "         [-0.4700,  1.3561, -0.0175,  ...,  0.9705,  0.7019,  0.6182],\n",
      "         [ 0.1531, -0.0253, -0.1894,  ...,  0.5447,  0.7327,  0.6182],\n",
      "         [ 0.7492,  1.5474,  0.8866,  ...,  0.4418,  0.6674,  0.8616],\n",
      "         [ 0.0071,  0.6976,  0.9401,  ...,  0.8413,  0.3559,  0.8616],\n",
      "         [-0.6347, -0.2307, -0.9765,  ..., -0.5059,  0.1007,  0.6182]]],\n",
      "       grad_fn=<ViewBackward>) torch.Size([1, 6, 768])\n",
      "\n",
      "============================== paddle =============================\n",
      "length: 6\n",
      "paddle size(shape) [6]\n",
      "Tensor(shape=[48], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 28, 20, 3, 0, 0, 0, 0, 0, 29, 14, 3, 0, 0, 0, 0, 0, 13, 26, 6, 19, 1, 0, 0, 0, 18, 6, 20, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Tensor(shape=[1, 6, 8], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[[0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [28, 20, 3 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [29, 14, 3 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [13, 26, 6 , 19, 1 , 0 , 0 , 0 ],\n",
      "         [18, 6 , 20, 1 , 0 , 0 , 0 , 0 ],\n",
      "         [0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]]) [1, 6, 8]\n",
      ">>>>paddle_pinyin_emb<<<<\n",
      "Tensor(shape=[1, 6, 768], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[[-0.06688929, -0.11022839,  0.22643268, ...,  0.24312425,  0.04734262, -0.31167182],\n",
      "         [ 0.13613178,  0.21606459,  0.22643268, ...,  0.24353099,  0.05083228, -0.01372745],\n",
      "         [ 0.13613178, -0.11022839,  0.22643268, ...,  0.24312425,  0.13102368,  0.06452615],\n",
      "         [ 0.16060145,  0.16710807,  0.22643268, ...,  0.24312425,  0.26675850, -0.02787493],\n",
      "         [ 0.09193362,  0.22377157,  0.22847086, ...,  0.24312425,  0.26675850, -0.09273362],\n",
      "         [-0.06688929, -0.11022839,  0.22643268, ...,  0.24312425,  0.04734262, -0.31167182]]]) [1, 6, 768]\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "print(\"============================== torch =============================\")\n",
    "sentence = '我喜欢猫'\n",
    "tokenizer = BertDataset(CHINESEBERT_PATH)\n",
    "input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)\n",
    "length = input_ids.shape[0]\n",
    "print(f\"length: {length}\")\n",
    "print(\"torch size:\", input_ids.size())\n",
    "print(pinyin_ids)\n",
    "pinyin_ids = pinyin_ids.view(1, length, 8)\n",
    "print(pinyin_ids, pinyin_ids.shape)\n",
    "\n",
    "\n",
    "hidden_size = 768\n",
    "config_path='./pretrain_models/torch/ChineseBERT-base/config/'\n",
    "pinyin_embeddings = PinyinEmbedding(embedding_size=128, \n",
    "                                    pinyin_out_dim=hidden_size,\n",
    "                                    config_path=config_path)\n",
    "\n",
    "torch_pinyin_emb = pinyin_embeddings(pinyin_ids)\n",
    "print(\">>>>torch_pinyin_emb<<<<\")\n",
    "print(torch_pinyin_emb, torch_pinyin_emb.shape)\n",
    "print()\n",
    "\n",
    "# paddle\n",
    "sentence = '我喜欢猫'\n",
    "paddle_tokenizer = PaddleBertDataset(CHINESEBERT_PATH)\n",
    "paddle_input_ids, paddle_pinyin_ids = paddle_tokenizer.tokenize_sentence(sentence)\n",
    "print(\"============================== paddle =============================\")\n",
    "length = paddle_input_ids.shape[0]\n",
    "print(f\"length: {length}\")\n",
    "print(\"paddle size(shape)\", paddle_input_ids.shape)\n",
    "print(paddle_pinyin_ids)\n",
    "paddle_pinyin_ids = paddle_pinyin_ids.reshape((1, length, 8))\n",
    "print(paddle_pinyin_ids, paddle_pinyin_ids.shape)\n",
    "\n",
    "hidden_size = 768\n",
    "config_path='./pretrain_models/torch/ChineseBERT-base/config/'\n",
    "paddle_pinyin_embeddings = PaddlePinyinEmbedding(embedding_size=128, \n",
    "                                                 pinyin_out_dim=hidden_size,\n",
    "                                                 config_path=config_path)\n",
    "paddle_pinyin_emb = paddle_pinyin_embeddings(paddle_pinyin_ids)\n",
    "print(\">>>>paddle_pinyin_emb<<<<\")\n",
    "print(paddle_pinyin_emb, paddle_pinyin_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "662ccf0e-d649-4ff6-9bc3-12eb38121dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch pinyin_embeddings: PinyinEmbedding(\n",
      "  (embedding): Embedding(32, 128)\n",
      "  (conv): Conv1d(128, 768, kernel_size=(2,), stride=(1,))\n",
      ")\n",
      "paddle paddle_pinyin_embeddings: PaddlePinyinEmbedding(\n",
      "  (embedding): Embedding(32, 128, sparse=False)\n",
      "  (conv): Conv1D(128, 768, kernel_size=[2], data_format=NCL)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch pinyin_embeddings: {pinyin_embeddings}\")\n",
    "print(f\"paddle paddle_pinyin_embeddings: {paddle_pinyin_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe539be-ca8b-4fa3-b1ab-29daf756c41e",
   "metadata": {},
   "source": [
    "## GlyphEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86afd63e-2b15-4b0d-95be-163b5d07e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "class GlyphEmbedding(nn.Module):\n",
    "    \"\"\"Glyph2Image Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, font_npy_files: List[str]):\n",
    "        super(GlyphEmbedding, self).__init__()\n",
    "        font_arrays = [\n",
    "            np.load(np_file).astype(np.float32) for np_file in font_npy_files\n",
    "        ]\n",
    "        self.vocab_size = font_arrays[0].shape[0]\n",
    "        self.font_num = len(font_arrays)\n",
    "        self.font_size = font_arrays[0].shape[-1]\n",
    "        # N, C, H, W\n",
    "        font_array = np.stack(font_arrays, axis=1)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.font_size ** 2 * self.font_num,\n",
    "            _weight=torch.from_numpy(font_array.reshape([self.vocab_size, -1]))\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "            get glyph images for batch inputs\n",
    "        Args:\n",
    "            input_ids: [batch, sentence_length]\n",
    "        Returns:\n",
    "            images: [batch, sentence_length, self.font_num*self.font_size*self.font_size]\n",
    "        \"\"\"\n",
    "        # return self.embedding(input_ids).view([-1, self.font_num, self.font_size, self.font_size])\n",
    "        return self.embedding(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904d0dfa-0592-4aef-b025-d3df8ae77133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "class PaddleGlyphEmbedding(paddle.nn.Layer):\n",
    "    \"\"\"Glyph2Image Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, font_npy_files: List[str]):\n",
    "        super(PaddleGlyphEmbedding, self).__init__()\n",
    "        font_arrays = [\n",
    "            np.load(np_file).astype(np.float32) for np_file in font_npy_files\n",
    "        ]\n",
    "        self.vocab_size = font_arrays[0].shape[0]\n",
    "        self.font_num = len(font_arrays)\n",
    "        self.font_size = font_arrays[0].shape[-1]\n",
    "        # N, C, H, W\n",
    "        font_array = np.stack(font_arrays, axis=1)\n",
    "        self.embedding = paddle.nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.font_size ** 2 * self.font_num\n",
    "        )\n",
    "        self.embedding.weight.set_value(font_array.reshape([self.vocab_size, -1]))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "            get glyph images for batch inputs\n",
    "        Args:\n",
    "            input_ids: [batch, sentence_length]\n",
    "        Returns:\n",
    "            images: [batch, sentence_length, self.font_num*self.font_size*self.font_size]\n",
    "        \"\"\"\n",
    "        # return self.embedding(input_ids).view([-1, self.font_num, self.font_size, self.font_size])\n",
    "        return self.embedding(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2777ff64-6b65-4e8d-99f4-5e1d4a215be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./pretrain_models/torch/ChineseBERT-base/config/STFANGSO.TTF24.npy', './pretrain_models/torch/ChineseBERT-base/config/STXINGKA.TTF24.npy', './pretrain_models/torch/ChineseBERT-base/config/方正古隶繁体.ttf24.npy']\n"
     ]
    }
   ],
   "source": [
    "config_path='./pretrain_models/torch/ChineseBERT-base/config/'\n",
    "font_files = []\n",
    "for file in os.listdir(config_path):\n",
    "    if file.endswith(\".npy\"):\n",
    "        font_files.append(os.path.join(config_path, file))\n",
    "print(font_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b1f26f9-4438-4be3-be37-2e93657c25c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== torch =============================\n",
      "torch glyph_embeddings: GlyphEmbedding(\n",
      "  (embedding): Embedding(23236, 1728)\n",
      ")\n",
      "============================== paddle =============================\n",
      "paddle paddle_glyph_embeddings: PaddleGlyphEmbedding(\n",
      "  (embedding): Embedding(23236, 1728, sparse=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch \n",
    "print(\"============================== torch =============================\")\n",
    "glyph_embeddings = GlyphEmbedding(font_npy_files=font_files)\n",
    "t_emb_w = glyph_embeddings.state_dict()['embedding.weight'].cpu().detach().numpy()\n",
    "print(f\"torch glyph_embeddings: {glyph_embeddings}\")\n",
    "\n",
    "# paddle\n",
    "print(\"============================== paddle =============================\")\n",
    "paddle_glyph_embeddings = PaddleGlyphEmbedding(font_npy_files=font_files)\n",
    "p_emb_w = paddle_glyph_embeddings.state_dict()['embedding.weight'].cpu().detach().numpy()\n",
    "print(f\"paddle paddle_glyph_embeddings: {paddle_glyph_embeddings}\")\n",
    "\n",
    "(p_emb_w == t_emb_w).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37cb0430-d8ab-4a82-a3c1-cd424cd1dc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding.weight'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle_glyph_embeddings.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aeab43-e25d-4114-989a-53c2454a0a66",
   "metadata": {},
   "source": [
    "## FusionBertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd48ab3d-66d3-4bf6-9c97-be1a26844957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "class FusionBertEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the embeddings from word, position, glyph, pinyin and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(FusionBertEmbeddings, self).__init__()\n",
    "        config_path = os.path.join(config.name_or_path, 'config')\n",
    "        font_files = []\n",
    "        for file in os.listdir(config_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                font_files.append(os.path.join(config_path, file))\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.pinyin_embeddings = PinyinEmbedding(embedding_size=128, pinyin_out_dim=config.hidden_size,\n",
    "                                                 config_path=config_path)\n",
    "        self.glyph_embeddings = GlyphEmbedding(font_npy_files=font_files)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow models variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.glyph_map = nn.Linear(1728, config.hidden_size)\n",
    "        self.map_fc = nn.Linear(config.hidden_size * 3, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids=None, pinyin_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        # get char embedding, pinyin embedding and glyph embedding\n",
    "        word_embeddings = inputs_embeds  # [bs,l,hidden_size]\n",
    "        pinyin_embeddings = self.pinyin_embeddings(pinyin_ids)  # [bs,l,hidden_size]\n",
    "        glyph_embeddings = self.glyph_map(self.glyph_embeddings(input_ids))  # [bs,l,hidden_size]\n",
    "        # fusion layer\n",
    "        concat_embeddings = torch.cat((word_embeddings, pinyin_embeddings, glyph_embeddings), 2)\n",
    "        inputs_embeds = self.map_fc(concat_embeddings)\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "715c74ba-f2fd-451e-aa7a-a4b6cfe5e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "import os\n",
    "\n",
    "import paddle\n",
    "class PaddleFusionBertEmbeddings(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    Construct the embeddings from word, position, glyph, pinyin and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(PaddleFusionBertEmbeddings, self).__init__()\n",
    "        config_path = os.path.join(config.name_or_path, 'config')\n",
    "        font_files = []\n",
    "        for file in os.listdir(config_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                font_files.append(os.path.join(config_path, file))\n",
    "        self.word_embeddings = paddle.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = paddle.nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = paddle.nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.pinyin_embeddings = PaddlePinyinEmbedding(embedding_size=128, \n",
    "                                                       pinyin_out_dim=config.hidden_size,\n",
    "                                                       config_path=config_path)\n",
    "        self.glyph_embeddings = PaddleGlyphEmbedding(font_npy_files=font_files)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow models variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.glyph_map = paddle.nn.Linear(1728, config.hidden_size, bias_attr=True)\n",
    "        self.map_fc = paddle.nn.Linear(config.hidden_size * 3, config.hidden_size, bias_attr=True)\n",
    "        self.LayerNorm = paddle.nn.LayerNorm(config.hidden_size, epsilon=config.layer_norm_eps)\n",
    "        self.dropout = paddle.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", paddle.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids=None, pinyin_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.shape\n",
    "        else:\n",
    "            input_shape = inputs_embeds.shape[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = paddle.zeros(input_shape, dtype='int64')\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        # get char embedding, pinyin embedding and glyph embedding\n",
    "        word_embeddings = inputs_embeds  # [bs,l,hidden_size]\n",
    "        pinyin_embeddings = self.pinyin_embeddings(pinyin_ids)  # [bs,l,hidden_size]\n",
    "        glyph_embeddings = self.glyph_map(self.glyph_embeddings(input_ids))  # [bs,l,hidden_size]\n",
    "        # fusion layer\n",
    "        concat_embeddings = paddle.concat((word_embeddings, pinyin_embeddings, glyph_embeddings), 2)\n",
    "        inputs_embeds = self.map_fc(concat_embeddings)\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1efa10c4-8e87-4bd8-ac74-eae11cd95fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"./pretrain_models/torch/ChineseBERT-base/\",\n",
       "  \"architectures\": [\n",
       "    \"GlyceBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 23236\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc7074fd-f851-4e5e-b387-bc6294ab66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== torch =============================\n",
      "FusionBertEmbeddings(\n",
      "  (word_embeddings): Embedding(23236, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (pinyin_embeddings): PinyinEmbedding(\n",
      "    (embedding): Embedding(32, 128)\n",
      "    (conv): Conv1d(128, 768, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (glyph_embeddings): GlyphEmbedding(\n",
      "    (embedding): Embedding(23236, 1728)\n",
      "  )\n",
      "  (glyph_map): Linear(in_features=1728, out_features=768, bias=True)\n",
      "  (map_fc): Linear(in_features=2304, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "============================== paddle =============================\n",
      "PaddleFusionBertEmbeddings(\n",
      "  (word_embeddings): Embedding(23236, 768, padding_idx=0, sparse=False)\n",
      "  (position_embeddings): Embedding(512, 768, sparse=False)\n",
      "  (token_type_embeddings): Embedding(2, 768, sparse=False)\n",
      "  (pinyin_embeddings): PaddlePinyinEmbedding(\n",
      "    (embedding): Embedding(32, 128, sparse=False)\n",
      "    (conv): Conv1D(128, 768, kernel_size=[2], data_format=NCL)\n",
      "  )\n",
      "  (glyph_embeddings): PaddleGlyphEmbedding(\n",
      "    (embedding): Embedding(23236, 1728, sparse=False)\n",
      "  )\n",
      "  (glyph_map): Linear(in_features=1728, out_features=768, dtype=float32)\n",
      "  (map_fc): Linear(in_features=2304, out_features=768, dtype=float32)\n",
      "  (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
      "  (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "print(\"============================== torch =============================\")\n",
    "torch_fusion_emb = FusionBertEmbeddings(config)\n",
    "print(torch_fusion_emb)\n",
    "\n",
    "# paddle\n",
    "print(\"============================== paddle =============================\")\n",
    "paddle_fusion_emb = PaddleFusionBertEmbeddings(config)\n",
    "print(paddle_fusion_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd6ed-8cf8-4a96-976c-45f6cdbaac0c",
   "metadata": {},
   "source": [
    "## GlyceBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32966a7e-1af2-40c5-8514-2b84ec334927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.modeling_bert import BertEncoder, BertPooler\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "\n",
    "class GlyceBertModel(BertModel):\n",
    "    r\"\"\"\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n",
    "            Sequence of hidden-states at the output of the last layer of the models.\n",
    "        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n",
    "            Last layer hidden-state of the first token of the sequence (classification token)\n",
    "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "            layer weights are trained from the next sentence prediction (classification)\n",
    "            objective during Bert pretraining. This output is usually *not* a good summary\n",
    "            of the semantic content of the input, you're often better with averaging or pooling\n",
    "            the sequence of hidden-states for the whole input sequence.\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the models at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        models = BertModel.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        outputs = models(input_ids)\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(GlyceBertModel, self).__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = FusionBertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        pinyin_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "            if the models is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask\n",
    "            is used in the cross-attention if the models is configured as a decoder.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, pinyin_ids=pinyin_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b82d21c4-bb2f-4977-83d1-337a42899aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "glyce_bert_model = GlyceBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7def816-cbd2-4e3d-8f73-34a913840e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.position_ids',\n",
       " 'embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.pinyin_embeddings.embedding.weight',\n",
       " 'embeddings.pinyin_embeddings.conv.weight',\n",
       " 'embeddings.pinyin_embeddings.conv.bias',\n",
       " 'embeddings.glyph_embeddings.embedding.weight',\n",
       " 'embeddings.glyph_map.weight',\n",
       " 'embeddings.glyph_map.bias']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glyce_bert_model.state_dict().keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99742219-cf5a-4865-9b71-114982e860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "import warnings\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.transformers import BertModel\n",
    "\n",
    "class BertPooler(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    Pool the result of BertEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, pool_act=\"tanh\"):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = paddle.nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = paddle.nn.Tanh()\n",
    "        self.pool_act = pool_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        if self.pool_act == \"tanh\":\n",
    "            pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class PaddleGlyceBertModel(BertModel):\n",
    "    r\"\"\"\n",
    "    PaddleGlyceBertModel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(PaddleGlyceBertModel, self).__init__(vocab_size=config.vocab_size)\n",
    "        self.embeddings = PaddleFusionBertEmbeddings(config)\n",
    "        encoder_layer = paddle.nn.TransformerEncoderLayer(\n",
    "            config.hidden_size,\n",
    "            config.num_attention_heads,\n",
    "            config.intermediate_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            activation=config.hidden_act,\n",
    "            attn_dropout=config.attention_probs_dropout_prob,\n",
    "            act_dropout=0)\n",
    "        self.encoder = paddle.nn.TransformerEncoder(encoder_layer, config.num_hidden_layers)\n",
    "        self.pooler = BertPooler(config.hidden_size)\n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        pinyin_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = paddle.unsqueeze(\n",
    "                (input_ids == self.pad_token_id\n",
    "                 ).astype(self.pooler.dense.weight.dtype) * -1e9,\n",
    "                axis=[1, 2])\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, pinyin_ids=pinyin_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        if output_hidden_states:\n",
    "            output = embedding_output\n",
    "            encoder_outputs = []\n",
    "            for mod in self.encoder.layers:\n",
    "                output = mod(output, src_mask=attention_mask)\n",
    "                encoder_outputs.append(output)\n",
    "            if self.encoder.norm is not None:\n",
    "                encoder_outputs[-1] = self.encoder.norm(encoder_outputs[-1])\n",
    "            pooled_output = self.pooler(encoder_outputs[-1])\n",
    "        else:\n",
    "            sequence_output = self.encoder(embedding_output, attention_mask)\n",
    "            pooled_output = self.pooler(sequence_output)\n",
    "        if output_hidden_states:\n",
    "            return encoder_outputs, pooled_output\n",
    "        else:\n",
    "            return sequence_output, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f8c13f6-c555-403e-bbcb-3e9dc9244602",
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle_glyce_bert_model = PaddleGlyceBertModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f493be03-40e5-49cc-b2f3-4b787ddcff7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.position_ids',\n",
       " 'bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.pinyin_embeddings.embedding.weight',\n",
       " 'bert.embeddings.pinyin_embeddings.conv.weight',\n",
       " 'bert.embeddings.pinyin_embeddings.conv.bias',\n",
       " 'bert.embeddings.glyph_embeddings.embedding.weight',\n",
       " 'bert.embeddings.glyph_map.weight',\n",
       " 'bert.embeddings.glyph_map.bias']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(paddle_state_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a63924-0b2a-4a41-8a02-a963c33a62ad",
   "metadata": {},
   "source": [
    "# 计算前项精度比较的方法\n",
    "* 两个框架对于同一个模型的前项输出，最大误差应该控制在 10^-4，即，说明复现成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8af09f6-3e19-421b-8692-4e28e937b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/ChineseBert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5e66f1-a526-4300-85ec-858f0bb4b5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2769, 1599, 3614, 4344,  102]])\n",
      "tensor([[[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [28, 20,  3,  0,  0,  0,  0,  0],\n",
      "         [29, 14,  3,  0,  0,  0,  0,  0],\n",
      "         [13, 26,  6, 19,  1,  0,  0,  0],\n",
      "         [18,  6, 20,  1,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0368, -0.0524, -0.1786,  ..., -0.1690,  0.1436,  0.0379],\n",
       "          [-0.1318,  0.1298,  0.3270,  ...,  0.2858, -0.5699,  0.2182],\n",
       "          [-0.3525, -0.0942, -0.1648,  ...,  0.2943, -0.7855, -0.0041],\n",
       "          [-0.1875,  0.0362,  0.1704,  ..., -0.3582, -0.2260, -0.1740],\n",
       "          [-0.0889,  0.3527, -0.2916,  ..., -0.0836, -0.1554,  0.2234],\n",
       "          [-0.0337, -0.0427, -0.1809,  ..., -0.1581,  0.1669,  0.0269]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " torch.Size([1, 6, 1024]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch\n",
    "from datasets.bert_dataset import BertDataset\n",
    "from models.modeling_glycebert import GlyceBertModel\n",
    "sentence = '我喜欢猫'\n",
    "CHINESEBERT_PATH='./pretrain_models/torch/ChineseBERT-large/'\n",
    "tokenizer = BertDataset(CHINESEBERT_PATH)\n",
    "chinese_bert = GlyceBertModel.from_pretrained(CHINESEBERT_PATH)\n",
    "chinese_bert.eval()\n",
    "input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)\n",
    "length = input_ids.shape[0]\n",
    "input_ids = input_ids.view(1, length)\n",
    "pinyin_ids = pinyin_ids.view(1, length, 8)\n",
    "print(input_ids)\n",
    "print(pinyin_ids)\n",
    "\n",
    "torch_output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[0]\n",
    "torch_output_hidden, torch_output_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4906051f-2fa0-4f48-8207-a55e27db4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chinese_bert.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd41569-072d-4494-9bbe-fed322c96c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/Paddle_ChineseBert/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6cd09a-5917-4e56-a549-9d12b0c7a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 21:05:51.689723 19563 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 6.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0913 21:05:51.693789 19563 device_context.cc:422] device: 0, cuDNN Version: 8.0.\n",
      "\u001b[32m[2021-09-13 21:05:58,713] [    INFO]\u001b[0m - Weights from pretrained model not used in GlyceBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 6], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[101 , 2769, 1599, 3614, 4344, 102 ]])\n",
      "Tensor(shape=[1, 6, 8], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[[0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [28, 20, 3 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [29, 14, 3 , 0 , 0 , 0 , 0 , 0 ],\n",
      "         [13, 26, 6 , 19, 1 , 0 , 0 , 0 ],\n",
      "         [18, 6 , 20, 1 , 0 , 0 , 0 , 0 ],\n",
      "         [0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1, 6, 1024], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
       "       [[[-0.03676526, -0.05237103, -0.17860234, ..., -0.16904099,  0.14357226,  0.03791038],\n",
       "         [-0.13184090,  0.12977326,  0.32704630, ...,  0.28580564, -0.56986696,  0.21823926],\n",
       "         [-0.35248795, -0.09423149, -0.16476731, ...,  0.29425690, -0.78550410, -0.00410738],\n",
       "         [-0.18750003,  0.03615668,  0.17038533, ..., -0.35818714, -0.22604686, -0.17404869],\n",
       "         [-0.08885855,  0.35273460, -0.29164734, ..., -0.08359668, -0.15540956,  0.22340605],\n",
       "         [-0.03365400, -0.04272101, -0.18089685, ..., -0.15809794,  0.16688700,  0.02686221]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paddle\n",
    "import paddle\n",
    "from paddlenlp.transformers import ChineseBertTokenizer\n",
    "from paddlenlp.transformers import GlyceBertModel\n",
    "sentence = '我喜欢猫'\n",
    "CHINESEBERT_PADDLE_PATH = \"./pretrain_models/paddle/ChineseBERT-large/\"\n",
    "tokenizer = ChineseBertTokenizer(CHINESEBERT_PADDLE_PATH)\n",
    "glyce_bert_model = GlyceBertModel.from_pretrained(CHINESEBERT_PADDLE_PATH)\n",
    "glyce_bert_model.eval()\n",
    "token_input = tokenizer.tokenize_sentence(sentence)\n",
    "input_ids = paddle.to_tensor(token_input['input_ids'])\n",
    "pinyin_ids = paddle.to_tensor(token_input['pinyin_ids'])\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "pinyin_ids = pinyin_ids.unsqueeze(0)\n",
    "print(input_ids)\n",
    "print(pinyin_ids)\n",
    "\n",
    "paddle_output_hidden = glyce_bert_model.forward(input_ids, pinyin_ids)[0]\n",
    "paddle_output_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62e0698-9ee5-4936-a67f-b2c76428fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大误差: 3.8146973e-06\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# torch\n",
    "t_output_hidden = torch_output_hidden.cpu().detach().numpy()\n",
    "\n",
    "# paddle\n",
    "p_output_hidden = paddle_output_hidden.cpu().detach().numpy()\n",
    "\n",
    "diff = t_output_hidden - p_output_hidden\n",
    "error = np.max(abs(diff))\n",
    "print(\"最大误差:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d45041-0cc7-4e82-8809-c9f50dd0f9db",
   "metadata": {},
   "source": [
    "# 附录\n",
    "## 对比 paddlenlp 和 huggingface 的 `bert-base-chinese` 模型\n",
    "* paddlenlp 文档：（这个文档有些老，但总体上是对的，需要注意 huggingface 现在模型参数已经弃用了 `attention.output.LayerNorm.gamma` 等）\n",
    "    https://paddlenlp.readthedocs.io/zh/latest/community/contribute_models/convert_pytorch_to_paddle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c063f3-0813-4673-bc2e-50762a55a2c9",
   "metadata": {},
   "source": [
    "### paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac09c721-f3ea-4360-ac86-0bb03644b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-09-12 16:08:40,516] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\u001b[0m\n",
      "\u001b[32m[2021-09-12 16:08:44,825] [    INFO]\u001b[0m - Weights from pretrained model not used in BertModel: ['cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.weight', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import BertModel\n",
    "\n",
    "p = BertModel.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9adfd633-b03d-4cc7-99ae-b7dd9b5feb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, sparse=False)\n",
       "    (position_embeddings): Embedding(512, 768, sparse=False)\n",
       "    (token_type_embeddings): Embedding(2, 768, sparse=False)\n",
       "    (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): LayerList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (8): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (9): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (10): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (11): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02bafb8f-fb79-4b89-9b74-38aadc1d66a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Tensor(shape=[768], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
       "       [0.99000758, 0.97690040, 0.97765732, 0.94424844, 0.76797146, 0.95842028, 0.95088869, 0.95098066, 0.89577252, 1.00088620, 0.98037577, 0.92581815, 0.83142120, 0.92646289, 0.95957601, 0.94461876, 0.99346662, 1.01123142, 0.96349865, 0.94775075, 0.98592800, 0.94116277, 1.01536751, 0.95520437, 0.94929725, 0.96389103, 0.96608067, 0.94282484, 1.01474190, 0.97430092, 0.91588789, 0.98019701, 0.99701631, 0.94673103, 0.99644357, 0.96920210, 0.97043175, 0.95901281, 0.94662303, 0.98728698, 0.97292870, 0.95352584, 0.97319037, 1.00273454, 0.92802316, 0.95470178, 0.97651005, 0.97335207, 0.94828159, 0.96513742, 0.97811615, 0.54728281, 0.95744258, 0.96712947, 0.97800148, 0.96688694, 0.96835792, 0.98429048, 0.90828168, 1.00500143, 0.98842162, 0.96154082, 0.94576639, 0.91822588, 1.00190949, 0.98568445, 0.96719068, 0.94651729, 0.94914740, 0.96677554, 0.93641907, 0.95245361, 0.96893132, 0.93815476, 0.99435675, 0.95183963, 0.94405425, 0.95714754, 0.84709013, 0.96410847, 0.99065012, 0.95382810, 0.96463341, 0.95254827, 0.94413954, 0.98671556, 0.97241217, 0.92539829, 0.92622894, 0.92748863, 0.93982971, 0.93800646, 0.94893390, 0.97191298, 0.92627245, 0.79239321, 0.93104881, 0.96533632, 0.93203562, 0.94945288, 0.83051479, 0.95363724, 0.97004414, 1.01279509, 0.94278276, 0.96320200, 0.97861433, 0.96069008, 0.96368897, 0.95923442, 1.00138950, 0.95845544, 0.97131121, 0.97262514, 0.94178540, 0.94133759, 0.99044055, 0.95117486, 0.97874987, 0.96201336, 0.96236658, 0.99169147, 0.96537095, 0.95120710, 0.96954244, 0.94109136, 0.97025210, 0.97911912, 0.96529132, 0.98389363, 0.97226095, 0.96066791, 0.95725602, 0.96732026, 0.96312952, 1.00388467, 0.95562482, 0.95968235, 1.00836599, 0.92824948, 0.97899634, 1.01749849, 0.97519827, 0.97117472, 0.88433212, 0.94862127, 0.95557582, 0.97153550, 0.95537406, 0.96103233, 0.96988219, 0.89507085, 0.92624658, 0.94378746, 0.95425075, 0.97733331, 0.97677302, 0.98012769, 0.95991617, 0.95962548, 0.98888159, 0.94667649, 0.99612433, 0.98793930, 0.95554638, 0.98182040, 0.99132138, 0.94752526, 0.97583872, 0.94682246, 0.88151938, 0.98055851, 0.94434869, 0.95001614, 0.94335353, 0.98447132, 0.96709037, 0.96535313, 0.75449568, 1.01910877, 0.98594910, 1.01089394, 0.97395730, 0.95492601, 0.92349231, 0.99792761, 0.94831896, 0.96529794, 0.76017934, 0.96214950, 0.95176035, 0.94354790, 0.89123118, 0.99498671, 0.94922549, 0.68578815, 0.90733218, 0.96609795, 0.87067997, 0.96501511, 0.92510706, 0.94799471, 1.00488639, 0.98282421, 0.92394727, 0.97323108, 0.99517137, 0.94262135, 0.95865101, 0.84395665, 0.97328442, 0.99634427, 0.98226744, 0.99597663, 0.99035358, 0.95704430, 0.96419758, 0.92023283, 0.94275928, 1.00412250, 0.93993604, 0.95052564, 0.96914119, 0.90356016, 0.96933639, 0.96225452, 0.97038615, 0.98184180, 0.94038242, 0.98998660, 0.97251260, 0.96104461, 0.98532355, 0.90799296, 0.99786425, 1.01137495, 0.97075194, 0.94238466, 0.96298736, 0.95420438, 0.97349226, 0.82428634, 0.93494797, 0.77532220, 0.95780343, 0.95940810, 0.91592139, 1.00425994, 0.97280431, 0.96575069, 0.98721665, 0.97345889, 0.91208458, 0.83979124, 0.95138431, 0.96196103, 0.94749540, 0.93033332, 0.98453462, 0.97155303, 0.78182244, 0.96778858, 0.97709078, 0.86765432, 0.99600786, 0.81490761, 0.97802037, 0.96649718, 0.95606339, 0.98066318, 0.97792596, 0.92190933, 0.97178358, 0.95670485, 0.97198808, 0.97579837, 0.99409324, 0.98584992, 0.93616104, 0.94029021, 1.00284576, 0.96626991, 0.98592693, 0.94886100, 0.93253893, 0.96337610, 0.94995964, 0.98920232, 0.90978235, 0.98159790, 0.98315376, 0.97478884, 0.81647539, 0.97325367, 0.35315087, 0.94920659, 0.99825764, 0.98572230, 0.97627932, 0.96366256, 0.98792416, 0.97944933, 0.97178984, 0.98053640, 0.90874887, 0.97205460, 0.95609140, 0.90164495, 0.99928743, 0.94877148, 0.93080395, 0.97474676, 0.99557573, 0.92818809, 0.93928140, 0.82723349, 0.94322926, 0.97130972, 0.95801973, 0.98875713, 1.00659871, 0.95872152, 0.99883795, 0.97309947, 0.99408716, 0.92873001, 0.97882295, 0.97151238, 0.97058678, 0.97399473, 0.93201888, 0.95366430, 0.94879514, 0.98308170, 0.92732400, 0.94687617, 0.98398280, 0.98085243, 0.97693825, 0.86049187, 0.98642826, 0.97533602, 0.97964615, 0.96886742, 0.95128137, 0.91625261, 0.96755344, 0.98397440, 0.99080050, 0.99837750, 0.98630530, 0.94460613, 0.92660534, 0.94326001, 0.98061365, 0.92639118, 0.85388052, 0.96665621, 0.96281332, 0.95701438, 0.97088480, 0.95867825, 0.93785858, 0.96003085, 0.78084522, 0.96698380, 0.93862468, 1.00194860, 0.76735085, 0.90241820, 0.95418864, 0.96103567, 0.95572197, 0.97503811, 0.94370788, 0.95982909, 0.95556504, 0.98886681, 0.96484655, 0.96228522, 0.98606068, 0.96612161, 0.93311685, 0.95724601, 0.94412529, 0.93899393, 0.98121065, 0.97286057, 0.81823999, 0.93164998, 0.90387601, 0.95466465, 0.96048516, 0.97500724, 0.98474008, 0.77134991, 0.94952440, 0.96818447, 0.94679010, 0.93905413, 0.94240749, 0.93016624, 0.96997839, 0.95320243, 0.96977639, 0.96815836, 0.98081511, 0.97561502, 0.97627139, 0.98873323, 0.98948765, 0.98243088, 0.80727106, 0.97104883, 0.98573768, 0.97171164, 0.98943508, 1.00085092, 0.80114323, 0.94870764, 0.94057524, 0.92879343, 0.98853141, 0.94567567, 0.93597549, 0.96972048, 0.99899024, 0.96571326, 0.96017164, 0.95227861, 0.97330052, 0.99428958, 0.97218609, 0.95463651, 1.00749862, 0.98282546, 0.97543573, 0.97670346, 0.95523906, 0.98900622, 0.93636245, 0.97299868, 0.97186518, 0.99651289, 0.97139460, 0.92282563, 0.95429474, 0.93972492, 0.93086743, 0.97957879, 0.99157351, 0.99941057, 0.97893149, 0.95010662, 0.96349698, 0.96534008, 0.96642870, 0.95064145, 0.95671296, 0.92560893, 0.97042221, 0.96352190, 0.99814141, 0.78394598, 0.97059959, 0.96195054, 0.96537733, 0.97114050, 0.93402201, 0.84394747, 0.96670491, 0.98209059, 0.94520146, 0.95746964, 0.82611847, 0.97785348, 0.98211801, 0.93825680, 1.03098750, 1.00725496, 0.96377975, 0.98252964, 0.96433336, 1.00309467, 1.04472947, 0.99095368, 0.93463463, 0.94936031, 0.97197610, 0.99034482, 0.97290140, 0.97405922, 0.97895664, 0.94819438, 0.95314777, 0.99884897, 0.98354805, 0.95983630, 0.97111541, 0.95497984, 0.96269542, 0.94405609, 0.97114295, 0.97058713, 0.98469120, 0.98117197, 0.95449680, 0.84672445, 0.95886421, 0.96014869, 0.95007700, 0.97190773, 0.96835816, 0.85269153, 0.96976501, 0.97744566, 0.98491126, 0.95628566, 0.95100600, 0.96165299, 0.95759022, 0.93243110, 0.97496796, 0.94477254, 0.98769796, 0.97831845, 0.79224408, 0.82100666, 0.82589322, 1.00489330, 0.96946102, 0.92238134, 0.95373845, 0.98170573, 0.99539739, 0.96254647, 0.93626916, 0.89613944, 0.91434103, 0.88526475, 0.96091086, 1.01007962, 0.98926240, 0.97958952, 0.95157617, 1.01426661, 0.96019036, 0.96106416, 0.96740925, 0.98409659, 0.97292131, 0.95160156, 0.99299681, 0.95875609, 0.96512759, 0.96131206, 0.97661728, 0.98507810, 0.95569462, 0.99057603, 0.95339084, 0.94662970, 0.98607177, 0.94543380, 0.95896500, 0.96702844, 0.94828671, 0.94251299, 0.94250387, 0.99568552, 0.79394585, 0.78627974, 0.95355898, 0.82919532, 0.94195908, 0.95772433, 0.93991619, 0.99203157, 0.91197360, 0.98795027, 0.99054307, 0.97175360, 0.95410466, 0.96652013, 0.94562882, 0.96638960, 0.98616743, 0.96800852, 0.93707001, 0.78887695, 0.90996790, 0.94246078, 0.97925740, 0.94857258, 0.98473328, 0.98063815, 0.95560908, 0.96960729, 0.97298825, 0.97397143, 0.96957171, 0.97819775, 0.98873204, 0.99044210, 0.97383213, 0.95655066, 0.99520016, 0.94612914, 0.94516134, 0.97436428, 0.91097045, 0.97898751, 0.93692517, 0.98336172, 0.98856544, 0.96806914, 0.76911229, 0.99314743, 0.96462220, 0.97589290, 0.98950773, 0.96501428, 0.97679156, 0.97038066, 0.97189957, 0.95595741, 0.84221601, 0.98332644, 0.96554577, 0.96921062, 0.96360946, 0.95460850, 0.99546951, 0.96629089, 0.75070500, 0.98504084, 0.96592838, 0.82031029, 0.98042333, 0.94555324, 0.97312218, 0.95599014, 0.96497476, 0.93560082, 0.93985426, 0.97713196, 0.96528423, 0.76711351, 0.95171434, 0.97181594, 0.94713497, 0.98967850, 0.80541974, 0.98331827, 0.98621869, 0.97070491, 0.97915781, 0.99740595, 0.96700364, 0.94879967, 0.68709475, 0.93670446, 0.99475634, 1.00134552, 0.95213622, 0.96652442, 0.92260939, 0.98214263, 0.97760773, 0.96631873, 0.97945720, 0.97404009, 0.95906675, 0.93739957, 0.99324161, 0.99098486, 0.99173957, 0.98236072, 0.95155931, 0.96430272, 0.96013802, 0.97018069, 0.97612816, 0.96731097, 1.02154791, 0.96169609, 1.00139391, 0.76714808, 0.75267541, 0.75680256, 0.97739577, 0.95656788, 0.97722644, 0.91803682, 0.94645625, 0.78798413, 0.97400767, 0.96455842, 0.94768000, 0.97880268, 0.98278528, 0.98147547, 0.77057594, 0.94923258, 0.97128350, 1.01699400, 0.94425064, 0.97467905, 0.98334438, 0.97756732, 0.96673781, 0.94034708, 0.93754566, 0.98039663, 0.94702756, 0.92719460, 0.92627132, 0.98541743, 0.90754378, 0.96025777, 0.98481691, 0.94878048, 0.99153888, 0.98695052, 0.97176218, 0.90386939, 0.97137153, 0.96779788, 0.97023797, 0.96096516, 0.96554434, 0.93458569, 0.92333633, 0.98129755, 0.93847895, 0.99785447, 0.96920007, 0.96188456, 1.00852227, 0.95413500, 0.96759266, 0.80151832, 0.97913080, 0.99269414, 0.95493060, 0.96071440, 0.94914842, 0.99712205, 0.95588744, 1.04973900, 0.97422802, 0.99454957, 0.95312631, 0.96672887, 0.97200036, 0.97435832, 0.60752112, 0.94129288, 0.94132811, 0.96519971, 0.97156376, 0.98671246, 0.94837481, 0.98220617, 0.97643685, 0.82210958, 0.94362414, 0.93605411])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.state_dict()['encoder.layers.1.norm2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e34b11b2-3952-417e-b0a3-8b8feea2de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight [21128, 768]\n",
      "embeddings.position_embeddings.weight [512, 768]\n",
      "embeddings.token_type_embeddings.weight [2, 768]\n",
      "embeddings.layer_norm.weight [768]\n",
      "embeddings.layer_norm.bias [768]\n",
      "encoder.layers.0.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.0.self_attn.q_proj.bias [768]\n",
      "encoder.layers.0.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.0.self_attn.k_proj.bias [768]\n",
      "encoder.layers.0.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.0.self_attn.v_proj.bias [768]\n",
      "encoder.layers.0.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.0.self_attn.out_proj.bias [768]\n",
      "encoder.layers.0.linear1.weight [768, 3072]\n",
      "encoder.layers.0.linear1.bias [3072]\n",
      "encoder.layers.0.linear2.weight [3072, 768]\n",
      "encoder.layers.0.linear2.bias [768]\n",
      "encoder.layers.0.norm1.weight [768]\n",
      "encoder.layers.0.norm1.bias [768]\n",
      "encoder.layers.0.norm2.weight [768]\n",
      "encoder.layers.0.norm2.bias [768]\n",
      "encoder.layers.1.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.1.self_attn.q_proj.bias [768]\n",
      "encoder.layers.1.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.1.self_attn.k_proj.bias [768]\n",
      "encoder.layers.1.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.1.self_attn.v_proj.bias [768]\n",
      "encoder.layers.1.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.1.self_attn.out_proj.bias [768]\n",
      "encoder.layers.1.linear1.weight [768, 3072]\n",
      "encoder.layers.1.linear1.bias [3072]\n",
      "encoder.layers.1.linear2.weight [3072, 768]\n",
      "encoder.layers.1.linear2.bias [768]\n",
      "encoder.layers.1.norm1.weight [768]\n",
      "encoder.layers.1.norm1.bias [768]\n",
      "encoder.layers.1.norm2.weight [768]\n",
      "encoder.layers.1.norm2.bias [768]\n",
      "encoder.layers.2.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.2.self_attn.q_proj.bias [768]\n",
      "encoder.layers.2.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.2.self_attn.k_proj.bias [768]\n",
      "encoder.layers.2.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.2.self_attn.v_proj.bias [768]\n",
      "encoder.layers.2.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.2.self_attn.out_proj.bias [768]\n",
      "encoder.layers.2.linear1.weight [768, 3072]\n",
      "encoder.layers.2.linear1.bias [3072]\n",
      "encoder.layers.2.linear2.weight [3072, 768]\n",
      "encoder.layers.2.linear2.bias [768]\n",
      "encoder.layers.2.norm1.weight [768]\n",
      "encoder.layers.2.norm1.bias [768]\n",
      "encoder.layers.2.norm2.weight [768]\n",
      "encoder.layers.2.norm2.bias [768]\n",
      "encoder.layers.3.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.3.self_attn.q_proj.bias [768]\n",
      "encoder.layers.3.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.3.self_attn.k_proj.bias [768]\n",
      "encoder.layers.3.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.3.self_attn.v_proj.bias [768]\n",
      "encoder.layers.3.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.3.self_attn.out_proj.bias [768]\n",
      "encoder.layers.3.linear1.weight [768, 3072]\n",
      "encoder.layers.3.linear1.bias [3072]\n",
      "encoder.layers.3.linear2.weight [3072, 768]\n",
      "encoder.layers.3.linear2.bias [768]\n",
      "encoder.layers.3.norm1.weight [768]\n",
      "encoder.layers.3.norm1.bias [768]\n",
      "encoder.layers.3.norm2.weight [768]\n",
      "encoder.layers.3.norm2.bias [768]\n",
      "encoder.layers.4.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.4.self_attn.q_proj.bias [768]\n",
      "encoder.layers.4.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.4.self_attn.k_proj.bias [768]\n",
      "encoder.layers.4.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.4.self_attn.v_proj.bias [768]\n",
      "encoder.layers.4.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.4.self_attn.out_proj.bias [768]\n",
      "encoder.layers.4.linear1.weight [768, 3072]\n",
      "encoder.layers.4.linear1.bias [3072]\n",
      "encoder.layers.4.linear2.weight [3072, 768]\n",
      "encoder.layers.4.linear2.bias [768]\n",
      "encoder.layers.4.norm1.weight [768]\n",
      "encoder.layers.4.norm1.bias [768]\n",
      "encoder.layers.4.norm2.weight [768]\n",
      "encoder.layers.4.norm2.bias [768]\n",
      "encoder.layers.5.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.5.self_attn.q_proj.bias [768]\n",
      "encoder.layers.5.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.5.self_attn.k_proj.bias [768]\n",
      "encoder.layers.5.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.5.self_attn.v_proj.bias [768]\n",
      "encoder.layers.5.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.5.self_attn.out_proj.bias [768]\n",
      "encoder.layers.5.linear1.weight [768, 3072]\n",
      "encoder.layers.5.linear1.bias [3072]\n",
      "encoder.layers.5.linear2.weight [3072, 768]\n",
      "encoder.layers.5.linear2.bias [768]\n",
      "encoder.layers.5.norm1.weight [768]\n",
      "encoder.layers.5.norm1.bias [768]\n",
      "encoder.layers.5.norm2.weight [768]\n",
      "encoder.layers.5.norm2.bias [768]\n",
      "encoder.layers.6.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.6.self_attn.q_proj.bias [768]\n",
      "encoder.layers.6.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.6.self_attn.k_proj.bias [768]\n",
      "encoder.layers.6.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.6.self_attn.v_proj.bias [768]\n",
      "encoder.layers.6.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.6.self_attn.out_proj.bias [768]\n",
      "encoder.layers.6.linear1.weight [768, 3072]\n",
      "encoder.layers.6.linear1.bias [3072]\n",
      "encoder.layers.6.linear2.weight [3072, 768]\n",
      "encoder.layers.6.linear2.bias [768]\n",
      "encoder.layers.6.norm1.weight [768]\n",
      "encoder.layers.6.norm1.bias [768]\n",
      "encoder.layers.6.norm2.weight [768]\n",
      "encoder.layers.6.norm2.bias [768]\n",
      "encoder.layers.7.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.7.self_attn.q_proj.bias [768]\n",
      "encoder.layers.7.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.7.self_attn.k_proj.bias [768]\n",
      "encoder.layers.7.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.7.self_attn.v_proj.bias [768]\n",
      "encoder.layers.7.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.7.self_attn.out_proj.bias [768]\n",
      "encoder.layers.7.linear1.weight [768, 3072]\n",
      "encoder.layers.7.linear1.bias [3072]\n",
      "encoder.layers.7.linear2.weight [3072, 768]\n",
      "encoder.layers.7.linear2.bias [768]\n",
      "encoder.layers.7.norm1.weight [768]\n",
      "encoder.layers.7.norm1.bias [768]\n",
      "encoder.layers.7.norm2.weight [768]\n",
      "encoder.layers.7.norm2.bias [768]\n",
      "encoder.layers.8.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.8.self_attn.q_proj.bias [768]\n",
      "encoder.layers.8.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.8.self_attn.k_proj.bias [768]\n",
      "encoder.layers.8.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.8.self_attn.v_proj.bias [768]\n",
      "encoder.layers.8.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.8.self_attn.out_proj.bias [768]\n",
      "encoder.layers.8.linear1.weight [768, 3072]\n",
      "encoder.layers.8.linear1.bias [3072]\n",
      "encoder.layers.8.linear2.weight [3072, 768]\n",
      "encoder.layers.8.linear2.bias [768]\n",
      "encoder.layers.8.norm1.weight [768]\n",
      "encoder.layers.8.norm1.bias [768]\n",
      "encoder.layers.8.norm2.weight [768]\n",
      "encoder.layers.8.norm2.bias [768]\n",
      "encoder.layers.9.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.9.self_attn.q_proj.bias [768]\n",
      "encoder.layers.9.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.9.self_attn.k_proj.bias [768]\n",
      "encoder.layers.9.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.9.self_attn.v_proj.bias [768]\n",
      "encoder.layers.9.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.9.self_attn.out_proj.bias [768]\n",
      "encoder.layers.9.linear1.weight [768, 3072]\n",
      "encoder.layers.9.linear1.bias [3072]\n",
      "encoder.layers.9.linear2.weight [3072, 768]\n",
      "encoder.layers.9.linear2.bias [768]\n",
      "encoder.layers.9.norm1.weight [768]\n",
      "encoder.layers.9.norm1.bias [768]\n",
      "encoder.layers.9.norm2.weight [768]\n",
      "encoder.layers.9.norm2.bias [768]\n",
      "encoder.layers.10.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.10.self_attn.q_proj.bias [768]\n",
      "encoder.layers.10.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.10.self_attn.k_proj.bias [768]\n",
      "encoder.layers.10.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.10.self_attn.v_proj.bias [768]\n",
      "encoder.layers.10.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.10.self_attn.out_proj.bias [768]\n",
      "encoder.layers.10.linear1.weight [768, 3072]\n",
      "encoder.layers.10.linear1.bias [3072]\n",
      "encoder.layers.10.linear2.weight [3072, 768]\n",
      "encoder.layers.10.linear2.bias [768]\n",
      "encoder.layers.10.norm1.weight [768]\n",
      "encoder.layers.10.norm1.bias [768]\n",
      "encoder.layers.10.norm2.weight [768]\n",
      "encoder.layers.10.norm2.bias [768]\n",
      "encoder.layers.11.self_attn.q_proj.weight [768, 768]\n",
      "encoder.layers.11.self_attn.q_proj.bias [768]\n",
      "encoder.layers.11.self_attn.k_proj.weight [768, 768]\n",
      "encoder.layers.11.self_attn.k_proj.bias [768]\n",
      "encoder.layers.11.self_attn.v_proj.weight [768, 768]\n",
      "encoder.layers.11.self_attn.v_proj.bias [768]\n",
      "encoder.layers.11.self_attn.out_proj.weight [768, 768]\n",
      "encoder.layers.11.self_attn.out_proj.bias [768]\n",
      "encoder.layers.11.linear1.weight [768, 3072]\n",
      "encoder.layers.11.linear1.bias [3072]\n",
      "encoder.layers.11.linear2.weight [3072, 768]\n",
      "encoder.layers.11.linear2.bias [768]\n",
      "encoder.layers.11.norm1.weight [768]\n",
      "encoder.layers.11.norm1.bias [768]\n",
      "encoder.layers.11.norm2.weight [768]\n",
      "encoder.layers.11.norm2.bias [768]\n",
      "pooler.dense.weight [768, 768]\n",
      "pooler.dense.bias [768]\n"
     ]
    }
   ],
   "source": [
    "for k in p.state_dict().keys():\n",
    "    print(k, p.state_dict()[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d8685-1fa9-45c3-83af-70de3aabfcbd",
   "metadata": {},
   "source": [
    "### huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c42688df-d17a-4fa3-bc3b-cacb558328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "h = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23e1f349-2d19-43df-90b3-6ff2214bd51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43aa2b13-f98f-4174-9e97-6f53012195a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1492e-01, -9.4048e-03,  5.8782e-03,  ...,  1.3729e-02,\n",
       "          1.5892e-02, -8.1710e-02],\n",
       "        [ 6.5357e-03, -2.2129e-02, -6.7798e-03,  ...,  4.7093e-05,\n",
       "          4.5497e-02,  1.3203e-02],\n",
       "        [ 1.3464e-02,  2.4955e-03,  3.5074e-02,  ...,  1.1137e-01,\n",
       "         -3.1568e-02, -1.4776e-02],\n",
       "        ...,\n",
       "        [-5.0526e-02,  2.3847e-02,  1.0480e-02,  ...,  5.6518e-02,\n",
       "          4.1081e-03,  7.1023e-02],\n",
       "        [ 1.7295e-02, -8.8264e-02, -5.6218e-02,  ..., -4.2443e-02,\n",
       "          3.8017e-02, -1.5388e-02],\n",
       "        [ 1.5026e-02, -2.9466e-02, -1.5802e-03,  ...,  9.7084e-02,\n",
       "         -3.4228e-02,  2.2910e-03]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.state_dict()['encoder.layer.0.attention.self.query.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5042c7ec-49a3-42cd-90c6-5fa5e8b0d2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.position_ids torch.Size([1, 512])\n",
      "embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k in h.state_dict().keys():\n",
    "    print(k, h.state_dict()[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dae9ce9-3299-43d6-86b4-7b606763de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-09-12 17:49:57,477] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\u001b[0m\n",
      "W0912 17:49:57.479743 31618 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 6.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0912 17:49:57.484134 31618 device_context.cc:422] device: 0, cuDNN Version: 8.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_kwargs: {'vocab_size': 21128, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'intermediate_size': 3072, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'pad_token_id': 0}\n",
      "----> init_class: BertModel\n",
      "---> cls: <class 'paddlenlp.transformers.bert.modeling.BertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-09-12 17:50:04,158] [    INFO]\u001b[0m - Weights from pretrained model not used in BertModel: ['cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.weight', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import BertModel\n",
    "bm = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb4e9df-2918-4cc4-8850-c1256cd05510",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.save_pretrained('./output/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bde145-a17f-48ca-bff5-5fea1cb2341b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
