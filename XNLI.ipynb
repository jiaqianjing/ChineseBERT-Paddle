{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56de7685-b564-4320-896c-423f70da087f",
   "metadata": {},
   "source": [
    "# XNLI\n",
    "1. train set 下载地址: \n",
    "2. dev and test set 下载地址: https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip (`xnli.dev.tsv`, `xnli.test.tsv`)\n",
    "3. 这里我已经将数据集下载到本地（`ChineseBERT-Paddle/data/XNLI`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11464d29-1dd1-44e4-983a-923ec64aaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/Paddle_ChineseBert/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a72904-5913-46cc-845b-172bcde30a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f07a98-fe9a-42b1-b8c5-7b1869f2a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read_train_ds(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        head = None\n",
    "        for no, line in enumerate(f, 1):\n",
    "            if no == 20001:\n",
    "                break\n",
    "            data = line.strip().split('\\t', 2)\n",
    "            if not head:\n",
    "                head = data\n",
    "            else:\n",
    "                sentence1, sentence2, label = data\n",
    "                yield {\"sentence1\": sentence1, \"sentence2\": sentence2, \"label\": label}\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        head = None\n",
    "        for no, line in enumerate(f, 1):\n",
    "            data = line.strip().split('\\t')\n",
    "            if not head:\n",
    "                head = data\n",
    "            else:\n",
    "                lan = data[0]\n",
    "                label = data[1]\n",
    "                sentence1 = data[6]\n",
    "                sentence2 = data[7]     \n",
    "                if lan != 'zh':\n",
    "                    continue\n",
    "                else:\n",
    "                    yield {\"sentence1\": sentence1, \"sentence2\": sentence2, \"label\": label}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6c7cf6-9bd2-4e7c-9f0e-876c78925858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read_train_ds, data_path='./data/XNLI/xnli.train.tsv', lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='./data/XNLI/xnli.dev.tsv', lazy=False)\n",
    "test_ds = load_dataset(read, data_path='./data/XNLI/xnli.test.tsv', lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f962207-540d-4bc3-b67d-2e5819696c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 2490, 5010)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(dev_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3b34fb-2685-4154-a2df-46aab8fe1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n",
      "3 2\n",
      "4 3\n",
      "5 4\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate([1,2,3,4], 2):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62b709a-da33-48fc-87c8-627eb167a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'sentence1': '从 概念 上 看 , 奶油 收入 有 两 个 基本 方面 产品 和 地理 .',\n",
       "   'sentence2': '产品 和 地理 是 什么 使 奶油 抹 霜 工作 .',\n",
       "   'label': 'neutral'},\n",
       "  {'sentence1': '你 知道 在 这个 季节 , 我 猜 在 你 的 水平 你 把 他们 丢到 下 一个 水平 , 如果 他们 决定 召回 的 家长 队 , 勇士 队 决定 打电话 召回 一个 家伙 从 三 个 a , 然后 一个 双人 上 去. 取代 他 和 一个 男人 去 取代 他',\n",
       "   'sentence2': '如果 人们 记得 的 话 , 你 就 会 把 事情 弄 丢 了 .',\n",
       "   'label': 'entailment'},\n",
       "  {'sentence1': '我们 的 一个 号码 会 非常 详细 地 执行 你 的 指示',\n",
       "   'sentence2': '我 团队 的 一个 成员 将 非常 精确 地 执行 你 的 命令',\n",
       "   'label': 'entailment'}],\n",
       " [{'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '校车把他放下后，他立即给他妈妈打了电话。',\n",
       "   'label': 'neutral'},\n",
       "  {'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '他没说一句话。',\n",
       "   'label': 'contradiction'},\n",
       "  {'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '他告诉他的妈妈他已经回到家了。',\n",
       "   'label': 'entailment'}],\n",
       " [{'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我还没有和他再次谈论。',\n",
       "   'label': 'contradiction'},\n",
       "  {'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我非常沮丧，我刚刚开始跟他说话。',\n",
       "   'label': 'entailment'},\n",
       "  {'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我们谈得很好。',\n",
       "   'label': 'neutral'}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3], test_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e690eb6e-e870-4cfe-a19f-abb067cc0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    \n",
    "    # 【FOCUS】 --> https://github.com/ShannonAI/ChineseBert/blob/main/datasets/xnli_dataset.py\n",
    "    label_map = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2, \"contradictory\": 2}\n",
    "    first, second, third = example['sentence1'], example['sentence2'], example['label']\n",
    "\n",
    "    first_input_tokens = tokenizer.tokenize(first)\n",
    "    first_pinyin_tokens = tokenizer.convert_sentence_to_pinyin_ids(first, with_specail_token=False)\n",
    "    \n",
    "    second_input_tokens = tokenizer.tokenize(second)\n",
    "    second_pinyin_tokens = tokenizer.convert_sentence_to_pinyin_ids(second, with_specail_token=False)\n",
    "\n",
    "    label = np.array([label_map[third]], dtype=\"int64\")\n",
    "    \n",
    "    # convert sentence to id\n",
    "    bert_tokens = tokenizer.convert_tokens_to_ids(first_input_tokens) + [102] + tokenizer.convert_tokens_to_ids(second_input_tokens)\n",
    "    pinyin_tokens = first_pinyin_tokens + [[0] * 8] + second_pinyin_tokens\n",
    "    if len(bert_tokens) > max_seq_length - 2:\n",
    "        bert_tokens = bert_tokens[:max_seq_length - 2]\n",
    "        pinyin_tokens = pinyin_tokens[:max_seq_length - 2]\n",
    "    # assert\n",
    "    assert len(bert_tokens) <= max_seq_length\n",
    "    assert len(bert_tokens) == len(pinyin_tokens)\n",
    "\n",
    "    input_ids = [101] + bert_tokens + [102]\n",
    "    pinyin_ids = [[0] * 8] + pinyin_tokens + [[0] * 8]\n",
    "    \n",
    "    input_ids = np.array(input_ids)\n",
    "    pinyin_ids = np.array(pinyin_ids)\n",
    "    \n",
    "\n",
    "    return input_ids, pinyin_ids, label\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # shuffle = False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, pinyin_ids, labels = batch\n",
    "        logits = model(input_ids, pinyin_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def do_train(model, tokenizer):\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # pinyin_ids\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    \n",
    "    test_data_loader = create_dataloader(\n",
    "        test_ds,\n",
    "        mode='test',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, pinyin_ids, labels = batch\n",
    "            logits = model(input_ids, pinyin_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            if global_step % 100 == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                print(\"dev eval:\")\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "                print(\"test eval:\")\n",
    "                evaluate(model, criterion, metric, test_data_loader)\n",
    "                # model._layers.save_pretrained(save_dir)\n",
    "                # tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47cb9d-51c4-4581-9b24-efa8c08fe52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=512, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=2, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0001, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=10, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ChineseBertModel\n",
    "CHINESEBERT_PADDLE_PATH = \"./pretrain_models/paddle/ChineseBERT-large\"\n",
    "model = ppnlp.transformers.GlyceBertForSequenceClassification.from_pretrained(CHINESEBERT_PADDLE_PATH, num_classes=4)\n",
    "\n",
    "# ChineseBertTokenizer\n",
    "tokenizer = ppnlp.transformers.ChineseBertTokenizer(CHINESEBERT_PADDLE_PATH)\n",
    "do_train(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad60b6-4cc2-450f-8eb0-db1feb05f754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
