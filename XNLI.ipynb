{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56de7685-b564-4320-896c-423f70da087f",
   "metadata": {},
   "source": [
    "# XNLI\n",
    "1. train set 下载地址: \n",
    "2. dev and test set 下载地址: https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip (`xnli.dev.tsv`, `xnli.test.tsv`)\n",
    "3. 这里我已经将数据集下载到本地（`ChineseBERT-Paddle/data/XNLI`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11464d29-1dd1-44e4-983a-923ec64aaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/Paddle_ChineseBert/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a72904-5913-46cc-845b-172bcde30a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f07a98-fe9a-42b1-b8c5-7b1869f2a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read_train_ds(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        head = None\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t', 2)\n",
    "            if not head:\n",
    "                head = data\n",
    "            else:\n",
    "                sentence1, sentence2, label = data\n",
    "                yield {\"sentence1\": sentence1, \"sentence2\": sentence2, \"label\": label}\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        head = None\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t')\n",
    "            if not head:\n",
    "                head = data\n",
    "            else:\n",
    "                lan = data[0]\n",
    "                label = data[1]\n",
    "                sentence1 = data[6]\n",
    "                sentence2 = data[7]     \n",
    "                if lan != 'zh':\n",
    "                    continue\n",
    "                else:\n",
    "                    yield {\"sentence1\": sentence1, \"sentence2\": sentence2, \"label\": label}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6c7cf6-9bd2-4e7c-9f0e-876c78925858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read_train_ds, data_path='./data/XNLI/xnli.train.tsv', lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='./data/XNLI/xnli.dev.tsv', lazy=False)\n",
    "test_ds = load_dataset(read, data_path='./data/XNLI/xnli.test.tsv', lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62b709a-da33-48fc-87c8-627eb167a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'sentence1': '从 概念 上 看 , 奶油 收入 有 两 个 基本 方面 产品 和 地理 .',\n",
       "   'sentence2': '产品 和 地理 是 什么 使 奶油 抹 霜 工作 .',\n",
       "   'label': 'neutral'},\n",
       "  {'sentence1': '你 知道 在 这个 季节 , 我 猜 在 你 的 水平 你 把 他们 丢到 下 一个 水平 , 如果 他们 决定 召回 的 家长 队 , 勇士 队 决定 打电话 召回 一个 家伙 从 三 个 a , 然后 一个 双人 上 去. 取代 他 和 一个 男人 去 取代 他',\n",
       "   'sentence2': '如果 人们 记得 的 话 , 你 就 会 把 事情 弄 丢 了 .',\n",
       "   'label': 'entailment'},\n",
       "  {'sentence1': '我们 的 一个 号码 会 非常 详细 地 执行 你 的 指示',\n",
       "   'sentence2': '我 团队 的 一个 成员 将 非常 精确 地 执行 你 的 命令',\n",
       "   'label': 'entailment'}],\n",
       " [{'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '校车把他放下后，他立即给他妈妈打了电话。',\n",
       "   'label': 'neutral'},\n",
       "  {'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '他没说一句话。',\n",
       "   'label': 'contradiction'},\n",
       "  {'sentence1': '他说，妈妈，我回来了。',\n",
       "   'sentence2': '他告诉他的妈妈他已经回到家了。',\n",
       "   'label': 'entailment'}],\n",
       " [{'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我还没有和他再次谈论。',\n",
       "   'label': 'contradiction'},\n",
       "  {'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我非常沮丧，我刚刚开始跟他说话。',\n",
       "   'label': 'entailment'},\n",
       "  {'sentence1': '嗯，我根本没想过，但是我很沮丧，最后我又和他说话了。',\n",
       "   'sentence2': '我们谈得很好。',\n",
       "   'label': 'neutral'}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3], test_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e690eb6e-e870-4cfe-a19f-abb067cc0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    \n",
    "    # 【FOCUS】 --> https://github.com/ShannonAI/ChineseBert/blob/main/datasets/xnli_dataset.py\n",
    "    label_map = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2, \"contradictory\": 2}\n",
    "    first, second, third = example['sentence1'], example['sentence2'], example['label']\n",
    "\n",
    "    first_input_tokens = tokenizer.tokenize(first)\n",
    "    first_pinyin_tokens = tokenizer.convert_sentence_to_pinyin_ids(first, with_specail_token=False)\n",
    "    \n",
    "    second_input_tokens = tokenizer.tokenize(second)\n",
    "    second_pinyin_tokens = tokenizer.convert_sentence_to_pinyin_ids(second, with_specail_token=False)\n",
    "\n",
    "    label = np.array([label_map[third]], dtype=\"int64\")\n",
    "    \n",
    "    # convert sentence to id\n",
    "    bert_tokens = tokenizer.convert_tokens_to_ids(first_input_tokens) + [102] + tokenizer.convert_tokens_to_ids(second_input_tokens)\n",
    "    pinyin_tokens = first_pinyin_tokens + [[0] * 8] + second_pinyin_tokens\n",
    "    if len(bert_tokens) > max_seq_length - 2:\n",
    "        bert_tokens = bert_tokens[:max_seq_length - 2]\n",
    "        pinyin_tokens = pinyin_tokens[:max_seq_length - 2]\n",
    "    # assert\n",
    "    assert len(bert_tokens) <= max_seq_length\n",
    "    assert len(bert_tokens) == len(pinyin_tokens)\n",
    "\n",
    "    input_ids = [101] + bert_tokens + [102]\n",
    "    pinyin_ids = [[0] * 8] + pinyin_tokens + [[0] * 8]\n",
    "    \n",
    "    input_ids = np.array(input_ids)\n",
    "    pinyin_ids = np.array(pinyin_ids)\n",
    "    \n",
    "\n",
    "    return input_ids, pinyin_ids, label\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # shuffle = False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, pinyin_ids, labels = batch\n",
    "        logits = model(input_ids, pinyin_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def do_train(model, tokenizer):\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    \n",
    "    test_data_loader = create_dataloader(\n",
    "        test_ds,\n",
    "        mode='test',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, pinyin_ids, labels = batch\n",
    "            logits = model(input_ids, pinyin_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            if global_step % 100 == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                print(\"dev eval:\")\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "                print(\"test eval:\")\n",
    "                evaluate(model, criterion, metric, test_data_loader)\n",
    "                # model._layers.save_pretrained(save_dir)\n",
    "                # tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47cb9d-51c4-4581-9b24-efa8c08fe52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.13474, accu: 0.25000, speed: 3.83 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 1.20751, accu: 0.25000, speed: 4.49 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 1.22180, accu: 0.23333, speed: 4.48 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 1.20309, accu: 0.25000, speed: 4.52 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 1.20132, accu: 0.29000, speed: 4.48 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 1.33859, accu: 0.29167, speed: 4.55 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.57701, accu: 0.30000, speed: 4.56 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.80777, accu: 0.31875, speed: 4.46 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 1.61620, accu: 0.32222, speed: 4.40 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 1.50649, accu: 0.31500, speed: 4.49 step/s\n",
      "dev eval:\n",
      "eval loss: 1.34011, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.34016, accu: 0.33333\n",
      "global step 110, epoch: 1, batch: 110, loss: 1.55079, accu: 0.45000, speed: 0.03 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 1.60250, accu: 0.37500, speed: 4.48 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 1.55349, accu: 0.36667, speed: 4.49 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 1.62389, accu: 0.38750, speed: 4.50 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 1.22895, accu: 0.38000, speed: 4.53 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 1.53590, accu: 0.37500, speed: 4.53 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 1.17893, accu: 0.36429, speed: 4.57 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 1.55590, accu: 0.36250, speed: 4.51 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.86779, accu: 0.35556, speed: 4.48 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.82595, accu: 0.36000, speed: 4.58 step/s\n",
      "dev eval:\n",
      "eval loss: 1.33837, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.33842, accu: 0.33333\n",
      "global step 210, epoch: 1, batch: 210, loss: 1.18713, accu: 0.35000, speed: 0.03 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 1.58071, accu: 0.25000, speed: 4.53 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 1.22677, accu: 0.28333, speed: 4.49 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 1.52880, accu: 0.26250, speed: 4.44 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 1.59352, accu: 0.28000, speed: 4.48 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 1.19734, accu: 0.27500, speed: 4.46 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 1.28571, accu: 0.28571, speed: 4.53 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 1.63221, accu: 0.30000, speed: 4.62 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 1.15103, accu: 0.30556, speed: 4.57 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.86900, accu: 0.31000, speed: 4.57 step/s\n",
      "dev eval:\n",
      "eval loss: 1.33515, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.33521, accu: 0.33333\n",
      "global step 310, epoch: 1, batch: 310, loss: 1.58860, accu: 0.20000, speed: 0.03 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 1.50315, accu: 0.30000, speed: 4.44 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 1.54736, accu: 0.26667, speed: 4.50 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 1.16511, accu: 0.28750, speed: 4.55 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 1.57871, accu: 0.30000, speed: 4.41 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 1.42633, accu: 0.34167, speed: 4.56 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 1.24406, accu: 0.35000, speed: 4.59 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 1.54208, accu: 0.34375, speed: 4.56 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 1.61690, accu: 0.33889, speed: 4.49 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 1.20835, accu: 0.35000, speed: 4.44 step/s\n",
      "dev eval:\n",
      "eval loss: 1.33134, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.33139, accu: 0.33333\n",
      "global step 410, epoch: 1, batch: 410, loss: 1.66153, accu: 0.40000, speed: 0.03 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 1.22615, accu: 0.32500, speed: 4.59 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 1.67949, accu: 0.33333, speed: 4.56 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 1.52540, accu: 0.28750, speed: 4.50 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 1.27782, accu: 0.27000, speed: 4.52 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 1.47004, accu: 0.28333, speed: 4.54 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 1.23499, accu: 0.27857, speed: 4.42 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 1.26934, accu: 0.30000, speed: 4.48 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 1.50866, accu: 0.30000, speed: 4.41 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 1.05301, accu: 0.30000, speed: 4.53 step/s\n",
      "dev eval:\n",
      "eval loss: 1.32573, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.32578, accu: 0.33333\n",
      "global step 510, epoch: 1, batch: 510, loss: 1.54719, accu: 0.45000, speed: 0.03 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 1.20276, accu: 0.37500, speed: 4.39 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 1.61383, accu: 0.33333, speed: 4.48 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 1.57889, accu: 0.31250, speed: 4.45 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 1.56478, accu: 0.29000, speed: 4.53 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 1.57341, accu: 0.30000, speed: 4.54 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 1.46184, accu: 0.30000, speed: 4.56 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 1.39953, accu: 0.28750, speed: 4.53 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.79163, accu: 0.30556, speed: 4.52 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 1.20239, accu: 0.29000, speed: 4.46 step/s\n",
      "dev eval:\n",
      "eval loss: 1.31875, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.31882, accu: 0.33333\n",
      "global step 610, epoch: 1, batch: 610, loss: 1.25603, accu: 0.50000, speed: 0.03 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 1.21280, accu: 0.35000, speed: 4.54 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 1.44180, accu: 0.35000, speed: 4.46 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 1.69645, accu: 0.37500, speed: 4.46 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 1.15810, accu: 0.38000, speed: 4.46 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.99568, accu: 0.35000, speed: 4.40 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 1.60426, accu: 0.34286, speed: 4.53 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 1.23729, accu: 0.33125, speed: 4.46 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 1.54565, accu: 0.34444, speed: 4.52 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 1.23282, accu: 0.35500, speed: 4.55 step/s\n",
      "dev eval:\n",
      "eval loss: 1.31203, accu: 0.33333\n",
      "test eval:\n",
      "eval loss: 1.31209, accu: 0.33333\n",
      "global step 710, epoch: 1, batch: 710, loss: 1.22132, accu: 0.30000, speed: 0.03 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 1.52123, accu: 0.30000, speed: 4.44 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 1.54168, accu: 0.30000, speed: 4.49 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 1.15101, accu: 0.32500, speed: 4.52 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 1.41204, accu: 0.31000, speed: 4.54 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 1.15871, accu: 0.32500, speed: 4.47 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 1.19663, accu: 0.33571, speed: 4.56 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 1.02575, accu: 0.33125, speed: 4.57 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 1.20877, accu: 0.35000, speed: 4.49 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 1.55946, accu: 0.36000, speed: 4.41 step/s\n",
      "dev eval:\n",
      "eval loss: 1.30491, accu: 0.33333\n",
      "test eval:\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=512, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=2, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0001, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=10, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ChineseBertModel\n",
    "CHINESEBERT_PADDLE_PATH = \"./pretrain_models/paddle/ChineseBERT-large\"\n",
    "model = ppnlp.transformers.GlyceBertForSequenceClassification.from_pretrained(CHINESEBERT_PADDLE_PATH, num_classes=4)\n",
    "\n",
    "# ChineseBertTokenizer\n",
    "tokenizer = ppnlp.transformers.ChineseBertTokenizer(CHINESEBERT_PADDLE_PATH)\n",
    "do_train(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7bb4a-1b9e-4ffd-b475-90c0277b1065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
