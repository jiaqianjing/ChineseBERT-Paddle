{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d2f12b-debb-47c0-8a68-efaaa260de6c",
   "metadata": {},
   "source": [
    "# ChnSetiCorp\n",
    "\n",
    "1. 本地目录数据：`ChineseBERT-Paddle/data/ChnSentiCorp` (测试集包含标签)\n",
    "2. 来源：https://github.com/pengming617/bert_classification/blob/master/data/test.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c4c428-0040-4a5d-85bd-d3948f6a7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/workspace/research/ChineseBERT-Paddle/Paddle_ChineseBert/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0abf1d7-1119-4397-9125-f72e07a0be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18075e80-2858-4beb-afe8-06bcbe3c6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        head = None\n",
    "        for line in f:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            if not head:\n",
    "                head = data\n",
    "            else:\n",
    "                label, text = data\n",
    "                yield {\"text\": text, \"label\": label, \"qid\": ''}\n",
    "\n",
    "# data_path为read()方法的参数\n",
    "train_ds = load_dataset(read, data_path='./data/ChnSentiCorp/train.tsv', lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='./data/ChnSentiCorp/dev.tsv', lazy=False)\n",
    "test_ds = load_dataset(read, data_path='./data/ChnSentiCorp/test.tsv', lazy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79097a21-0a91-45ba-93dd-cf86f6490cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "   'label': '1',\n",
       "   'qid': ''},\n",
       "  {'text': '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错',\n",
       "   'label': '1',\n",
       "   'qid': ''},\n",
       "  {'text': '房间太小。其他的都一般。。。。。。。。。', 'label': '0', 'qid': ''}],\n",
       " [{'text': '這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ 中餐廳的廣東點心不太好吃~~要改善之~~~~但算價錢平宜~~可接受~~ 西餐廳格調都很好~~但吃的味道一般且令人等得太耐了~~要改善之~~',\n",
       "   'label': '1',\n",
       "   'qid': ''},\n",
       "  {'text': '<荐书> 推荐所有喜欢<红楼>的红迷们一定要收藏这本书,要知道当年我听说这本书的时候花很长时间去图书馆找和借都没能如愿,所以这次一看到当当有,马上买了,红迷们也要记得备货哦!',\n",
       "   'label': '1',\n",
       "   'qid': ''},\n",
       "  {'text': '商品的不足暂时还没发现，京东的订单处理速度实在.......周二就打包完成，周五才发货...',\n",
       "   'label': '0',\n",
       "   'qid': ''}],\n",
       " [{'text': '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般', 'label': '1', 'qid': ''},\n",
       "  {'text': '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！',\n",
       "   'label': '0',\n",
       "   'qid': ''},\n",
       "  {'text': '还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。',\n",
       "   'label': '0',\n",
       "   'qid': ''}])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3], test_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53db337b-f177-4657-8ead-bfd58308465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    # 【FOCUS】 --> https://github.com/ShannonAI/ChineseBert/blob/f6b4cd901e8f8b3ef2340ce2a8685b41df9bc261/datasets/chn_senti_corp_dataset.py#L33\n",
    "    text = example[\"text\"][:max_seq_length-2]\n",
    "    encoded_inputs = tokenizer.tokenize_sentence(text)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    pinyin_ids = encoded_inputs[\"pinyin_ids\"]\n",
    "\n",
    "    if is_test:\n",
    "        return input_ids, pinyin_ids\n",
    "    label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "    return input_ids, pinyin_ids, label\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # shuffle = False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, pinyin_ids, labels = batch\n",
    "        logits = model(input_ids, pinyin_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def do_train(model, tokenizer):\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    \n",
    "    test_data_loader = create_dataloader(\n",
    "        test_ds,\n",
    "        mode='test',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, pinyin_ids, labels = batch\n",
    "            logits = model(input_ids, pinyin_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            if global_step % 100 == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                print(\"dev eval:\")\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "                print(\"test eval:\")\n",
    "                evaluate(model, criterion, metric, test_data_loader)\n",
    "                # model._layers.save_pretrained(save_dir)\n",
    "                # tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af514f6d-eae4-4719-a8ad-1427d6ce2eac",
   "metadata": {},
   "source": [
    "## ChineseBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f25236-872e-4820-b4a7-e360dbf312be",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=512, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=2, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0001, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=10, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ChineseBertModel\n",
    "CHINESEBERT_PADDLE_PATH = \"./pretrain_models/paddle/ChineseBERT-base\"\n",
    "model = ppnlp.transformers.GlyceBertForSequenceClassification.from_pretrained(CHINESEBERT_PADDLE_PATH, num_class=2)\n",
    "\n",
    "# ChineseBertTokenizer\n",
    "tokenizer = ppnlp.transformers.ChineseBertTokenizer(CHINESEBERT_PADDLE_PATH)\n",
    "do_train(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec5df9-d748-4ce2-9165-69bfd04a52f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
